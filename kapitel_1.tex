\chapter{Lineare Programme}
\section{Einleitung}
\begin{definition}
Seien 
\begin{itemize}
\item $A$ eine ganzzahlige $m\times n$-Matrix mit den Zeilenvektoren $a_i^T$
\item $\{M_1,M_2\}$ eine Partition von $\laue{m}$
\item $\{N_1,N_2\}$ eine Partition von $\laue{n}$
\item $x \in \mathbb R^n, b,c \in \mathbb Z^n$
\end{itemize}
Dann heißt das Problem $\min(c^Tx = \sum_{i=1}^m c_ix)$ mit den Randbedingungen
\begin{align*}
a_i^Tx &= b_i \; &\forall i \in M_1 \\
a_i^Tx &\geq b_i \; &\forall i \in M_2 \\
x_j &\geq 0 \; &\forall j \in N_1 \\
x_j &\gtrless 0 \; &\forall j \in N_2 \\
\end{align*}
\Begriff{allgemeines Lineares Programm} oder kurz allgemeines LP-Problem.\newline %
Für $M_1 = N_2 = \emptyset$ ist es in \Begriffx{kanonische Form}{kanonischer Form}.\newline %
Für $M_2 = N_2 = \emptyset$ ist es in \Begriff{Standardform}.
\end{definition}

\begin{theorem}
Alle drei Formen des LP-Problems sind äquivalent ( d.h. können ineinander überführt werden ).
\end{theorem}
\begin{proof}
Da LP-Probleme in kanonischer und Standardform automatisch auch allgemeine LP-Probleme sind, reicht es zu zeigen, dass jedes allgemeine LP-Problem in ein Problem in kanonischer und Standardform überführt werden kann.
\begin{enumerate}
\item{\textit{allgemeine in kanonische Form}} \newline
Die Umwandlung erfolgt durch Ersetzen von Bedingungen durch äquivalente Bedingungen. Dabei muss das Gleichungssystem (Matrix) um zusätzliche Gleichungen (Zeilen) und Variablen (Spalten) erweitert werden.
Alle Randbedingungen der Form $a_i^Tx = b_i$ sind äquivalent zu $a_i^Tx \geq b_i \land -a_i^Tx \geq -b_i$. Die Matrix erweitert sich folglich um $\lvert M_1 \rvert$ Zeilen. Für alle Randbedingungen der Form $x_j \gtrless 0$ definiere zwei Variablen:
\begin{align*}
 x_j^{+} &\colon= \begin{cases} %
x_j & \text{für } x_j > 0 \\
0 &\text{sonst}
\end{cases} \\
 x_j^{-} &\colon= \begin{cases} %
-x_j & \text{für } x_j < 0 \\
0 &\text{sonst}
\end{cases}
\end{align*}
Folglich gilt $x_j = x_j^{+} - x_j^{-}$. Die Bedingung $x_j \gtrless 0$ ist folglich äquivalent zu $x_j^{+} \geq 0 \land x_j^{-} \geq 0$. Die Matrix erweiter sich also nochmals um $\lvert N_1 \rvert$ Spalten (da wir eine Variable mehr haben) und der Vektor $x$ um ebensoviele Komponenten. Das entstandene Problem ist in kanonischer Form.
\item{\textit{allgemeine in Standardform}} \newline
siehe Übung
\end{enumerate}
\end{proof}

\section{Standard Form}
Wir betrachten zunächst ein Problem in Standardform unter folgenden Vorraussetzungen:
\begin{enumerate}[(a)]
	\item $rang(A) = m$\label{vorr:a}
	\item die Menge der zulässigen Punkte $F = \{x| Ax=b, x\geq0\}$ ist nicht leer\label{vorr:b}
	\item die Menge der Werte der Zielfunktion $\{c^T|x \in F\}$ ist nach unten beschränkt\label{vorr:c}
\end{enumerate}
\begin{definition}
Eine \Begriff{Basis} $B$ von $A$ ist eine Auswahl von linear unabhängigen Spaltenvektoren aus $A$. Alternativ entspricht $B$ einer regulären $m\times n$-Matrix.
Die \Begriff{Basislösung} zu $B$ ist ein Vektor $x\in \mathbb R^n$ mit 
\begin{align*}
x_{j_k} &= \begin{cases}
t_k &\text{wenn }A_{j_k} \in B \\
0 &\text{sonst}
\end{cases}\\
t &= B^{-1}b
\end{align*}
\end{definition}

Eine Basislösung $x$ zu einer Matrix $A$ können wir wie folgt berechnen:
\begin{enumerate}
\item Wähle eine Basis $B$ von $A$
\item Setze alle Komponenten von $x$, die nicht zu den gewählten Spalten aus $B$ gehören auf $0$.
\item Löse das resultierende Gleichungssystem, um die restlichen Komponenten von $x$ zu bestimmen.
\end{enumerate}

\begin{lemma}
Es sei $x = (x_1,\dotsc,x_n)$ eine Basislösung, $\alpha = \max\limits_{i\in \laue{m},j\in \laue{n}}\{\lvert a_{i,j} \rvert\}$ und $\beta = \max\limits_{j\in \laue{n}}\{\lvert b_j \rvert\}$. Dann gilt $\lvert x_j \rvert \leq m!\alpha^{m-1}\beta$ und $x_j \in \mathbb Q$.
\end{lemma}
\begin{proof}
Für eine Nicht-Basiskomponente gilt die Aussage, da diese per Definition $0$ sind. Die Basiskomponente $x_j$ ist die Summe von $m$ Produkten von Elementen von $B^{-1}$ und $b$. Nach Definition der Inversen gilt:
\begin{align*}
B^{-1} = \frac{1}{\det B}\adj(B) &= \frac{1}{\det B} \begin{pmatrix}
\mathcal B_{1,1} & \dots & \mathcal B_{1,m} \\
\vdots & \ & \vdots \\
\mathcal B_{m,1} & \dots & \mathcal B_{m,m}
\end{pmatrix}
\end{align*}
wobei $\mathcal B_{i,j}$ das Produkt von $(-1)^{i+j}$ und der Determinanten der Matrix ist, die durch Streichen der $i$-tem Zeile und $j$-ten Spalte aus $B$ entsteht.
Aufgrund des Entwicklungssatzes für Determinanten ist $\mathcal B_{i,j}$ die Summe von $(m-1)!$ Produkten von $m-1$ Elementen aus $A$. Somit gilt:
\begin{align*}
\lvert \mathcal B_{i,j} \rvert &\leq (m-1)!\alpha^{m-1} &&\text{da alle Elemente von $A$ $\leq \alpha$ sind}\\
\lvert x_j \rvert &= \left \lvert  \sum_{i \in \laue{m}} B^{-1}_{i,j} b_i \right \rvert &&\text{nach Definition}\\
&=\frac{1}{\det B}\left \lvert  \sum_{i \in \laue{m}} \mathcal B_{i,j} b_i \right \rvert \\
&\leq\frac{1}{\det B} \left \lvert  \sum_{i \in \laue{m}} (m-1)!\alpha^{m-1} b_i \right \rvert\\
&=\frac{1}{\det B}\left \lvert  m \cdot (m-1)!\alpha^{m-1} \cdot \beta \right \rvert \\
&=\frac{1}{\det B} m \cdot (m-1)!\alpha^{m-1} \cdot \beta &&\text{da alle Faktoren $\geq 0$ sind}
\end{align*}
Da $\det B$ ganzzahlig ist, folgt $\det B \geq 1$.
\end{proof}

\begin{theorem}
Unter der Vorraussetzungen \ref{vorr:a} und \ref{vorr:b} existert mind. eine Basislösung.
\end{theorem}
\begin{proof}
\textbf{Annahme:} Es existiert eine Lösung $x \in F$ mit $t > m$ Nicht-Null-Komponenten und es gibt keine Lösung $x' \in F$ mit weniger Nicht-Null-Komponenten.\newline\newline
Wir können O.B.d.A. annehmen, dass die ersten $t$ Komponenten von $x$ größer als $0$ sind und die restlichen Komponenten gleich $0$ sind (wenn nicht können wir dies durch Vertauschung erreichen).
Es gilt also $x_1,\dotsc,x_t > 0$ und $x_{t+1}, \dotsc,x_n=0$.
Hierraus folgt:
\begin{align}
b &= A_1x_1 + \dotsc + A_nx_n = A_1x_1 + \dotsc + A_tx_t \label{eq:x_n_t}
\end{align}
Es sei nun $r$ der Rang der Matrix $\lbrack A1, \dotsc, A_t\rbrack$. Wenn $r = 0$ wäre, so wäre $\vec 0$ eine zulässige Basislösung mit weniger Nicht-Null-Komponenten als $x$. Damit die Annahme stimmen kann muss also $0 < r \leq m < t$ gelten.
Mit dem Gauß-Jordan-Verfahren lassen sich nun die ersten $r$ Zeilen und Spalten in eine reguläre Form überführen. Das Problem sieht nun wie folgt aus:
\begin{align*}
\begin{pmatrix}
1 & & 0 & -\overline{a}_{1,r+1} &\hdots & -\overline{a}_{1,t} \\
 & \ddots & & \vdots & \ddots & \vdots \\
0 & & 1 & -\overline{a}_{r,r+1} &\hdots & -\overline{a}_{r,t} \\
? &&& \hdots && ?\\
\vdots &&&&&\\
?&&&&&\\
\end{pmatrix}\begin{pmatrix}
x_1\\
\vdots\\
x_r\\
\vdots
\end{pmatrix}&=\begin{pmatrix}
\overline{b}_1\\
\vdots\\
\overline{b}_r\\
\vdots
\end{pmatrix}
\end{align*}

Es lässt sich nun \eqref{eq:x_n_t} schreiben als:
\begin{align}
x_j &= \overline{b_j} + \sum_{i=r+1}^t \overline{a_{j,i}} x_i  &\text{für } j &={1,\dotsc,r}
\end{align}
Setze $\Theta = \min( x_{r+1}, \Theta_1)$ mit $\Theta_1 = \min( \frac{x_j}{\overline{a}_{j,r+1}}, j=1,\dotsc,r, \overline{a}_{j,r+1}>0 )$.
Konstruiere eine neue Lösung $\hat{x}$:
\begin{align}
\hat{x}_j &= \begin{cases}
\overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x_i} & \text{für } j < r+1 \\
x_j - \Theta & \text{für }  j=r+1\\
x_j & \text{für }  j>r+1
\end{cases}
\end{align}
Diese erfüllt das Gleichungssystem.
Dann gilt für $j<r$:
\begin{align}
\hat{x}_j &= \overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x}_i && j < r+1 \\
&= \overline{b}_j + \hat{a}_{j,r+1}(x_{r+1} - \Theta ) + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x}_i && j < r+1 \\
&= \underbrace{ \overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i}\hat{x}_i }_{x_j} - \Theta\overline{a}_{j,r+1}
\end{align}
Ist $\Theta = x_{r+1}$, so ist $\hat{x}_{r+1} = 0$. Ist $\Theta = \Theta_1 = \frac{x_k}{\overline{a}_{k,r+1}}$. mit $k \leq r$, so ist %
 $\hat{x}_k = x_k - \Theta \overline{a}_{k,r+1} = 0$.
Für die Zulässigkeit ist noch zu zeigen: $\hat{x}_j \geq 0 \forall j \leq r + 1$.
Sehen wir uns $\hat{x}_{r+1}$ an:
Nach Definition: $\hat{x}_{r+1} = x_{r+1} - \Theta \geq \Theta - \Theta = 0$
\begin{align}
\hat{x}_j &= x_j - \Theta\overline{a}_{j,r+1} \\
&\geq \begin{cases}
 x_j + \Theta \lvert \overline{a}_{j,r+1} \rvert > 0 & \overline{a}_{j,r+1} \leq 0 \\
 x_j - \frac{x_j}{\overline{a}_{j,r+1}} \overline{a}_{j,r+1} = 0 & \overline{a}_{j,r+1} > 0
\end{cases}
\end{align}
Insgesamt ist $\hat{x}$ eine zulässige Lösung mit einer Nicht-Nullkomponente weniger als $x$, was ein Widerspruch zur Annahme ist.
\end{proof}
Es gibt also eine Lösung $x \in F$ mit $t\leq m$ Nicht-Nullkomponentent. O.B.d.A sind die zugehörigen Spalten linear unabhängig ( ansonsten Arg. oben mit Elimination von Variablen bzw. Gleich 0 setzen wiederholen ).
Falls $t < m$ ist, erweitere die Spalten zu einer Basis für $x$. ( Austauschsatz von Steinitz )

\begin{theorem}
Unter der Vorraussetzungen \ref{vorr:a},\ref{vorr:b} und \ref{vorr:c} ist das Lineare Programm $\min(c^Tx), Ax=b, x \geq 0$ äquivalent zu $\min(c^Tx), Ax=b, x \geq 0, x\leq M$ wobei %
$M = (m+1)!\alpha^m\beta$ mit $\alpha = max(\lvert a_{i,j}\rvert,\lvert c_{j}\rvert), \beta = max(\lvert b_i\rvert, \lvert z\rvert)$ und $z$ die Größte untere Schranke von $\{c^Tx | Ax=b, x \geq 0\}$ ist. 
\end{theorem}

\begin{quote}
In diesem Fall ist äquivalent zu verstehen als Gleichheit der Lösungsmengen. Es ist also jede Lösung des ersten Problems auch eine Lösung des zweiten Problems und umgekehrt.
\end{quote}

\section{Geometrie von Linearen Programmen}

Ein \Begriff{linearer Teilraum} S des $\mathbb R^d$ ist eine Teilmenge von $\mathbb R^b$, die
bezüglich der Vektoraddition und Skalarmultiplikation abgeschlossen ist.
Ein \Begriff{affiner Teilraum} A des $\mathbb R^d$ ist ein linearer Teilraum $S$,
verschoben um einen Vektor $u \in \mathbb R^d$: $A=\{u+x|x \in S\}$.

Die Dimension eines linearen Teilraumes S ist gleich der maximale Zahl von linear unabhängigen
Vektoren in S. Ebenso bei affinen Teilräumen. Die Dimension irgendeiner Menge X ist die kleinste
Dimension eines affinen Teilraums, der X enthält. Äquivalent können wir einen affinen bzw. linearen
Teilraum des $\mathbb R^d$ wie folgt darstellen:
$A = \{ x \in \mathbb R^d | a_i^Tx = b_i , 1 \leq i \leq m \}$
bzw. $S = \{ x \in \mathbb R^d | a_i^Tx = 0 , 1 \leq i \leq m \}$.

Zum Beispiel hat eine Kante die Dimension 1 und ein Menge von k Punkten höchstens die Dimension k-1.
Ein affiner Teilraum des $\mathbb R^d$ der Dimension $d-1$ wird als \Begriff{Hyperebene} bezeichnet.
Alternativ ist dies eine Menge von Punkten $H=\{x \in \mathbb R^d |a^Tx=b\}$ mit $a \neq 0$.
Eine Hyperebene definiert zwei \Begriff{abgeschlossene Halbräume}
$H^{+} = \{x \in \mathbb R^d |a^Tx \geq b\}$ und $H^{-} = \{x \in \mathbb R^d |a^Tx \leq b\}$.
Der Durchschnitt von endl. vielen Halbräumen wird bezeichnet als ein \Begriff{Polyeder}.
Ein Polyeder heißt \Begriff{Polytop}, wenn es beschränkt ist.

Es sei $P$ ein Polytop der Dimension $d$ im $\mathbb R^d$ und es sein $H \in \mathbb R^d$ eine Hyperebene. So ist $F = P \cap H$ eine Seitenfläche von $P$, wenn $H$ mindestens einen Punkt gemeinsam mit $P$ hat und wenn $P$ in höchstens einem der beiden Halbräumen $H^{+}$ und $H^{-}$ liegt. Es gibt drei wichtige Fälle:
\begin{enumerate}
\item eine \Begriff{Facette}, d.h. eine Seitenfläche der Dimension $d-1$.
\item eine \Begriff{Ecke}, d.h. eine Seitenfläche der Dimension $0$.
\item eine \Begriff{Kante}, d.h. eine Seitenfläche der Dimension $1$.
\end{enumerate}
Beispiel: ein Würfel im $\mathbb R^3$ ist gegeben durch $P = \{(x_1,x_2,x_3) | 0 \leq x_i \leq 1, i=1,2,3\}$. Dieser Würfel hat 6 Facetten, 8 Ecken und 12 Kanten.
\begin{align*}
F_1 &= P \cap \{(x_1,x_2,x_3) | x_3 = 1 \} && \text{Facette} \\
F_2 &= P \cap \{(x_1,x_2,x_3) | x_1 - x_3 = 1 \} && \text{Kante} \\
F_3 &= P \cap \{(x_1,x_2,x_3) | x_1 + x_2 + x_3 = 1 \} && \text{Ecke}
\end{align*}
\subsection{Umwandlung}
\begin{theorem}
\label{theorem:polytop_huelle}
\begin{enumerate}
\item Jedes Polytop ist die konvexe Hülle seiner Ecken.
\item Ist $V$ eine endliche Menge von Punkten/Vektoren, so ist die konvexe Hülle von $V$ ein Polytop.
\end{enumerate}
\end{theorem}
\begin{definition}
\Begriff{konvexe Hülle}
\begin{align*}
conv(V) &= \Big \{ \sum_{i=1}^k \lambda_i v_i \Big | v_i \in V, \lambda_i \geq 0, \sum_{i=1}^k \lambda_i = 1 \Big \}
\end{align*}
\end{definition}
Wir sind hauptsächlich interessiert an Polytopne, die im positiven Orthanten liegen.
D.h. die ersten d-Halbräume sind definiert durch $x_j \geq 0 ( j = 1,\dotsc,d)$.
Wegen \ref{theorem:polytop_huelle} kann ein Polytop $P$ auf verschiedene Weisen dargestellt werden:
\begin{enumerate}[a)]
\item als konvexe Hülle einer endlichen Punktemenge
\item als Durchschnitt von endlich vielen Halbräumen (der zusätzlich beschränkt ist)
\item als zulässiger Bereich eines LP-Problems (algebraische Darstellung)
\end{enumerate}
\paragraph*{Umwandlung LP-Problem in Durchschnitt von Halbräumen}
Es sei $F = \{ x | Ax = b , x \geq 0 \}$ der zulässige Bereich eines LP-Problems. Wir setzen vorraus, dass die Bedingungen \ref{vorr:a},\ref{vorr:b} und \ref{vorr:c} erfüllt sind. Da $rang(A) = m$ gilt, können wir O.B.d.A. annehmen, dass die Gleichungen $Ax=b$ in der folgenden Form vorliegen:
\begin{align}
x_{i+n-m} &= b_i - \sum_{j=1}^{n-m} a_{i,j}x_j & i = 1,\dotsc,m
\end{align}
Die Umformung erfolgt über Gauß-Jordan. Die Matrix enthält also einen Einheitsmatrix-Bereich im rechts-oberen Teil. Wegen der Vorraussetzung, dass $x_{i+n-m} \geq 0$ ist $Ax=b, x \geq 0$ äquivalent zu:
\begin{align}
 b_i - \sum_{j=1}^{n-m} a_{i,j}x_j &\geq 0 & i = 1,\dotsc,m\\
 x_j &\geq 0 & i = 1,\dotsc,n-m
\end{align}
Das ist der Durchschnitt von $n$ Halbräumen und ein Polytop im $\mathbb R^{n-m}$.
\paragraph*{Umwandlung  Durchschnitt von Halbräumen in LP-Problem}
Die $n$ Halbräume, die $P$ bestimmen seinen
\begin{align*}
h_{i,1} x_1 + \dotsc + h_{i,n-m} x_{n-m} &\leq g_i & i = 1,\dotsc,n
\end{align*}
Nach unserer Vorraussetzung sind die ersten $n-m$ Ungleichungen von der Form $x_i \geq 0, i = 1,\dotsc,n-m$.
Verwende positive Schlupfvariablen $x_{n-m+1}, \dotsc , x_n$ und erhalte:
\begin{align*}
h_{i,1} x_1 + \dotsc + h_{i,n-m} x_{n-m} + x_i &= g_i & i = n-m+i,\dotsc,n
\end{align*} 
Nun kann man dieses Problem als LP-Problem schreiben. Bilde dazu $A = ( H, I )$ (ein $m\times n$-Matrix) und erhalte das folgende System:
\begin{align*}
A &= \begin{pmatrix}
h_{1,1} & \hdots & h_{1,n-m} & 1 && 0\\
\vdots & \ddots & \vdots && \ddots & \\
h_{i,1} & \hdots & h_{i,n-m} & 0 && 1
\end{pmatrix}\\
Ax &= b\\
x &\geq 0\\
b &= (g_{n-m+1},\dotsc,g_n)^T
\end{align*}
\begin{definition}\label{def:umwandlung_punkt_vektor}
Der $x$-Vektor in den Halbraumdurchschnitten hatte nur die Dimension $n-m$, der $x$-Vektor des LP-Problems hatte die Dimension $n$. 
Jeder Punkt $\hat{x} \in (x_1,...,x_{n-m})^T \in P$ kann transformiert werden zu einem Punkt aus $F$ mit $n$ Komponenten. Dazu muss man lediglich die Schlupfvariablen ausrechnen:
\begin{align*}
x_i &= g_i - \sum_{j=1}^{n-m} h_{i,j}x_j & \text{für $i=n-m+1,\dotsc,n$}
\end{align*}
Ungekehrt wird ein $x\in F$ durch weglassen der letzten $m$ Komponenten zu einem Punkt $\hat{x} \in P$.
\end{definition}
\begin{theorem}
Es sei $P$ ein nicht-leeres Polytop, $F=\{x | Ax = b, x\geq 0\}$ der zugehörige zulässige Bereich eines LP und $\hat{x} = (x_1,\dotsc,x_{n-m})^T \in P$. Dann sind äquivalent:
\begin{enumerate}[a)]
\item Der Punkt $\hat{x}$ ist eine Ecke von $P$.
\item Wenn $\hat{x} = \lambda \hat{x}^1 + (1-\lambda)\hat{x}^2$ mit $\hat{x}^1,\hat{x}^2 \in P$ und $0 < \lambda < 1$, dann gillt $\hat{x} = \hat{x}^1 = \hat{x}^2$.
\item Der zugeordnete Vektor $x \in \mathbb R^n$, definiert durch \ref{def:umwandlung_punkt_vektor} ist eine zulässige Basislösung von $F$. \label{th:basisloesung_eq_ecke}
\end{enumerate}
\end{theorem}
\begin{proof}

\textbf{a $\Rightarrow$ b:} siehe Übung

\textbf{b $\Rightarrow$ c:}
$\hat{x}$ habe die Eigenschaft $b$.
Betrachte den zugeordneten Punkt $x \in F$ und die Menge $B= \{A_j | x_j > 0; j=1,\dotsc,n \}$.
Wir wollen zeigen, dass $B$ eine linear unabhängige Menge von Spalten ist. \newline
\textit{Annahme:} $B$ ist eine linear abhängige Menge von Spaltenvektoren.\newline
Dann gibt es Zahlen $\lambda_j$ ( nicht alle 0 ) mit $\sum_{A_j \in B} \lambda_j A_j = 0$. Da $x \in F$ gilt, gilt  $\sum_{A_j \in B} x_j A_j = b$ und $x_j \geq 0$. Dann gilt für jedes $\theta \in \mathbb R$:
\begin{align*}
\sum_{A_j \in B} (x_j \pm \theta\lambda_j) A_j &= b
\end{align*}
Da $x_j > 0$ für alle $A_j \in B$, gibt es ein $\theta > 0$ mit $x_j \pm \theta \lambda_j \geq 0 \forall A_j \in B$. Dann gibt es aber zwei Punkte $x^1, x^2 \in F$, definiert durch:
\begin{align*}
x_j^i &= \begin{cases}
x_j + (-1)^i \theta \lambda & A_j \in B \\
0 &\text{sonst}
\end{cases}
& i=1,2
\end{align*}
Da gilt:
\begin{align*}
x &= \frac{1}{2} x^1 + \frac{1}{2} x^2 
\end{align*}
Für die zugeordneten Punkte $\hat{x}^1, \hat{x}^2 \in P$ gilt $
\hat{x} = \frac{1}{2} \hat{x}^1 + \frac{1}{2} \hat{x}^2$, da diese durch Weglassung von Komponenten entstehen. Jedoch gilt $\hat{x}^1 \neq \hat{x}^2$. Also ist $B$ eine Menge von linear unabhängigen Spaltenvektoren mit $\lvert B \rvert \leq m$ und kann zu einer Basis erweitert werden.

\textbf{c $\Rightarrow$ a:}
Wenn $y = (y_1, \dotsc, y_n)^T$ eine zulässige Basislösung von $Ax = b, x  \geq 0$ ist, so gibt es einen Kostenvektor $c$, so dass $y$ der eindeutige Lösungsvektor des Systems $c^Tx \leq c^Ty, Ax=b, x\geq 0$ ist. Wir benutzen \ref{def:umwandlung_punkt_vektor} um hierraus Bedingungen für Punkte aus dem Polytop $P$ zu gewinnen:
\begin{align*}
c^Tx &= \sum_{i=1}^n c_ix_i \\
&=\sum_{i=1}^{n-m} c_ix_i + \sum_{i=n-m+1}^n c_ix_i \\
&=\sum_{i=1}^{n-m} c_ix_i + \sum_{i=n-m+1}^n c_i \left ( g_i - \sum_{j=1}^{n-m} h_{i,j}x_j \right )
\end{align*}
Fassen wir die Koeffizienten von $x_j$ jeweils zusammen, so bekommen wir:
\begin{align*}
d_j &= c_j - \sum_{i=n-m+1}^n c_i h_{i,j}
\end{align*}
Es folgt dann, dass $\hat{y} = (y_1,\dotsc,y_{n-m})$ der eindeutige Punkt im $\mathbb R^{n-m}$ ist mit
\begin{align*}
d^T\hat{x} &\leq d^T\hat{y} \forall \hat{x} \in P
\end{align*}
Also ist $\hat{x}$ eine Ecke von $P$.
\begin{quotation}
Die Menge der $\hat{x}$, für die $d^T\hat{x} = d^T\hat{y}$ gilt, bilden eine Hyperebene. Da der Schnittpunkt dieser Hypereben mit $P$ exakt einen Punkt hat ( nämlich $\hat{y}$ ) und $P$ komplett auf einer Seite dieser Hyperebene liegt ( nämlich $d^T\hat{x} \geq d^T\hat{y} \forall \hat{x} \in P$ ), ist $\hat{y}$ eine Ecke vom $P$. 
\end{quotation}
\end{proof}

\section{Transformation von einer zulässigen Basislösung zu einer anderen}
Es sei $x^0$ eine zulässige Basislösung eines LP-Problems mit gegebener $m\times n$-Matrix $A$ und zugeordneter Basis $B$ mit
\begin{align*}
B&=\{A_{B(i)} | i=1,\dotsc,m\}\\
B(i) &\in \{1,\dotsc,n\} \text{geordnet}
\end{align*}
Die Basiskomponenten von $x^0$ seien mit $x_i^0$ bezeichnet. Dann folgt:
\begin{align*}
\sum_{i=1}^m x_i^0 A_{B(i)} = b , x_i^0 \geq 0
\end{align*}
Da die Spalten der Basis linear unabhängig sind, kann jede andere Spalte $A_j$, die nicht in der Basis ist, als nicht-triviale Kombination der Basisspalten dargestellt werden. Es gilt also:
\begin{align*}
\sum_{i=1}^m x_{i,j} A_{B(i)} &= A_j
\end{align*}
Beide Gleichungen ergeben mit $\theta > 0$:
\begin{align*}
\sum_{i=1}^m ( x_i^0 - \theta x_ij ) A_{B(i)} + \theta A_j &= b
\end{align*}
\begin{definition}
Eine Basislösung heißt \Begriff{entartet}, wenn sie eine Nullkomponente enthält.
\end{definition}
Wenn $x^0$ nicht entartet ist, gilt $x_i^0 > 0$ für $i=1,\dotsc,m$. Mit 
\begin{align}
\theta_0 &= \min\limits_{i \text{mit} x_{i,j} > 0} \frac{x_i^0}{x_{i,j}} \\
&= \frac{x_l^0}{x_{l,j}} \label{eq:min_austausch}
\end{align}
wird ( falls mindestens ein $x_{i,j} > 0$ ist ) mindestens eine Komponente $0$ gesetzt und die Zulässigkeit bleibt erhalten.

\paragraph*{Spezialfall 1:} $x^0$ ist entartet und das zugehörige $x_{i,j} > 0$, dann folgt: $\theta_0 = 0$. In diesem Fall ändert sich die Lösung nicht, obwohl man die Spalte $B(i)$ durch die Spalte $A_j$ ersetzen kann.

\paragraph*{Spezialfall 2:} Alle $x_{i,j} \leq 0$. In diesem Fall kann $\theta$ ohne den Verlust der Zulässigkeit beliebig groß gesetzt werden. D.h. $F$ ist unbeschränkt.

\paragraph*{Spezialfall 3:} Wenn das Minimum in \eqref{eq:min_austausch} für mehrere Indices angenommen wird, so ist die neue Basislösung entartet.

\begin{theorem}
Gegeben sei eine zulässige Basislösung $x^0$ mit Basiskomponenten $x_i^0$, ($i=1,\dotsc,m$) und Basis $B=\{A_{B(i)} | i=1,\dotsc,m\}$ und ein Index $j$ sodass $A_j \notin B$. Dann ist die neue zulässige Lösung, die durch \eqref{eq:min_austausch} und durch
\begin{align}
x_i^1 &= \begin{cases}
x_i^0 - \theta_0 x_{i,j} & i \neq j\\
\theta_0 & i = j\end{cases}
\end{align}
bestimmt ist, eine zulässige Basislösung mit
\begin{align*}
B' &= (B \setminus \{A_{B(l)}\} ) \cup \{A_j\}
\end{align*} vorrausgesetzt mindestens ein $x_{i,j} > 0$ existiert.
\end{theorem}
\begin{example}
\begin{align*}
3x_1 + 2x_2 + x_3 &= 1\\
5x_1 + x_2 + x_3 + x_4 &= 3\\
2x_1 + 5x_2 + x_3 + x_5 &= 4
\end{align*}
Dargestellt als Tableau:
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
1 & 3 & 2 & 1 & 0 & 0\\
3 & 5 & 1 & 1 & 1 & 0\\
4 & 2 & 5 & 1 & 0 & 1
\end{matrix}
\end{align*}
Durch elementare Zeilenumformungen erhalten wir eine Einheitsmatrix und damit eine zulässige Basislösung.
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
1 & 3 & 2 & 1 & 0 & 0\\
2 & 2 & -1 & 0 & 1 & 0\\
3 & -1 & 3 & 0 & 0 & 1
\end{matrix}
\end{align*}
\begin{quotation}
Die Werte der nullten Spalte wird werden mit $x_{j,0}$ bezeichnet.
\end{quotation}
Nun hat man als Basis $B = \{A_3, A_4, A_5\}$ und die $0$-te Spalte gibt die Werte der Basisvariablen an. Die Nicht-Basisspalten in dem Tableau enthalten genau die Zahlen $x_{i,j}$, so zu Beispiel
\begin{align*}
A_1 &= 3A_3 + 2 A_4 - 1 A_5\\
 &=\sum x_{i,j} A_{B(i)}
\end{align*}
Die notwendigen Berechnungen für den Basiswechsel können direkt in dem Tableau ausgeführt werden. Wenn wir zum Beispiel die Spalte $A_1$ in die Basis aufnehmen wollen, so bestimmen wir das $\theta_0$ mit \eqref{eq:min_austausch}:
\begin{align*}
\theta_0 &= \min \left ( \frac{1}{3}, \frac{2}{2} \right ) = \frac{1}{3} \\
l&= 1
\end{align*}
Wir ersetzen die Spalte $B(l) = B(1) = 3$ mit Spalte $j=1$, indem wir in Spate $1$ einen Einheitsvektor $e_1$ erzeugen. Das neue Tableau sieht wie folgt aus:
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
\frac{1}{3} & 1 & \frac{2}{3} & \frac{1}{3} & 0 & 0\\
\frac{4}{3} & 0 & -\frac{7}{3} & -\frac{2}{3} & 1 & 0\\
\frac{4}{3} & 0 & \frac{11}{3} & \frac{1}{3} & 0 & 1
\end{matrix}
\end{align*}
Die neue Basis ist nun $B' = \{ A_1, A_4, A_5 \}$. Wenn im allgemeinen $x_{i,j}$ und $x'_{i,j}$ die alten und neuen Tableauwerte sind, $B$ und $B'$ die alte und neue Basis und $x_{l,j}$ das Pivotelement ist, so sehen Updateformeln wie folgt aus:
\begin{align*}
x_{l,q}' &= \frac{x_{l,q}}{x_{l,j}} &\text{für } q=0,\dotsc,n \\
x_{i,q}' &= x_{i,q} - x'_{l,q} x_{i,j} &\text{für } i=1,\dotsc,m; i \neq l ; q = 0,\dotsc,n \\
B'(i) &= \begin{cases}
B(i) & \text{für } i \neq l \\
j & \text{für } i = l
\end{cases}
\end{align*}
\begin{quotation}
\textbf{Bem:} $B(i)$ besagt, wo der Vektor $e_i$ im Tableau steht.
\end{quotation}
\end{example}

\subsection{Effekt der Kostenänderung}
Es sein $x^0$ ein zulässige Basislösungund Kosten $z^0 = \sum_{i=1}^m x_i^0 c_{B(i)}$. Die Kostenänderung, für den Fall, dass die Spalte $A_j \notin B$ in die Basis gebracht wird, sieht wie folgt aus:
\begin{align*}
\sum_{i=1}^m (\underbrace{ x_i^0 - \theta x_ij }_{x_i^1}) A_{B(i)} + \underbrace{\theta}_{x_j^1} A_j &= b
\end{align*}
Für jede Einheit, die $x_j$ zusätzlich bekommt, muss ein Betrag von $x_{i,j}$ der Variablen $x_{B(i)}$ weggelassen für $i=1,\dotsc,m$. Die Kostenänderung,
\begin{align}
\overline{c_j} &= c_j - \sum_{i=1}^m x_{i,j} c_{B(i)} = c_j - z_j \label{eq:rel_kosten}
\end{align}
wird \Begriff{relative Kosten} der Spalte $j$ genannt. Dabei ist es günstig eine Spalte $j$ mit $\overline{c_j} < 0$ in die Basis aufzunehmen. Wenn alle $\overline{c} \geq 0$ sind, haben wir ein lokales Optimum gefunden. Dieses lokale Optimum ist sogar ein globales (wird später bewiesen).
Für irgendein Tableau $X$ sei $B$ die $m \times m$-Matrix, die die Spalten aus $A$ für die Basis in enthalten und es sei $c_B \in \mathbb Z^m$ die Kosten dieser Basisvariablen. Da wir $X$ durch diagonalisierung der Basisspalten von $A$ bekommen, gilt, dass $X=B^{-1}A$ und für den Vektor $z=(z_1,\dotsc,z_n)$ gilt nach \eqref{eq:rel_kosten}:
$z^T = c_B^TX = c_B^TB^{-1}A$.
\begin{theorem}
Bei gegebener zulässiger Basislösung $x^0$ ändern sich die Kosten, wenn die Variable $x_j$ durch einen Pivotschritt in die Basis aufgenommen wird, um den Betrag
\begin{align*}
\theta_0 \overline{c_j} = \theta_0 (c_j - z_j)
\end{align*}
\end{theorem}
Wenn $\overline{c} = c - z \geq 0$, so ist $x^0$ optimal.

Die relativen Kosten $\overline{c_j}$ fpr ein Basisspalte $j$:
\begin{align*}
\overline{c_j} &= c_j - \sum_{i=1}^m \underbrace{x_{i,j}}_{\text{= 1 nur für $j=B(i)$}} c_{B(i)} \\
&= c_{B(i)} - c_{B(i)} = 0
\end{align*}
Die anfängliche $0$. Zeile (gegeben durch den Kostenwerte $(0,c_1,\dotsc,c_m)$ ) können wir in den Komponenten der Basisspalte zu $0$ setzen in dem wir die $i$-te Zeil multiplizieren mit $-c_{B(i)}$ und zu $0$-ten Zeile addieren.
Dies liefert in den Nicht-Basisspalten:
\begin{align*}
c_j - \sum_{i=1}^m c_{B(i)} x_{i,j} &= \overline{c_j}
\end{align*}
und in der $0$-ten Spalte:
\begin{align*}
0 - \sum_{i=1}^m c_{B(i)} x_{i,0} &= -z^0 &\text{(neg Kostenwert der Basislösung $x^0$)}
\end{align*}
Wenn wir die $0$-te Zeile analog zu den Update-Formeln mitmodifizieren:
\begin{align*}
x'_{0,q} &= x_{0,q} - x'_{l,q}x_{0,j} & q = 0,\dotsc,n
\end{align*}
so können wir die relativen Kosten und den negativen Kostenwert aus dem Tableau direkt ablesen.

\section{Simplex-Algorithmus}
\begin{verbatim}
procedure simplex;
begin
	opt = false, unbeschränkt = false // ist eine der beiden variablen wahr, wird abgebrochen
	while( (opt=false) and (unbeschränkt=false) ) do
		if $\overline{c_j} = x_{0,j} \geq 0$ für alle j then
			opt = true
		else
			wähle ein $j$ mit $\overline{c_j} = x_{0,j} < 0$
			if( $x_{i,j} \leq 0$ für alle $i$ ) then
				unbeschränkt = true	
			else
				bestimme $\theta_0 = \min\limits_{i:x_{i,j} > 0} \frac{x_{i,0}}{x_{i,j}} = \frac{x_{l,0}}{x_{l,j}}$
				// das Pivotelement ist dann $x_{l,j}$
				pivotisiere nach $x_{l,j}$
			end
		end
	end
end
\end{verbatim}
Bei diesem Verfahren wird vorrausgesetzt, dass wir eine zulässige Anfangslösung haben.
\begin{example}
\begin{align*}
\begin{matrix}
0 & 1 & 1 & 1 & 1 & 1 \\
1 & 3 & 2 & 1 & 0 & 0 \\
3 & 5 & 1 & 1 & 1 & 0 \\
4 & 2 & 5 & 1 & 0 & 1
\end{matrix}&\longrightarrow\begin{matrix}
-6 & -3 & -3 & 0 & 0 & 0 \\
1 & 3 & \underline{2} & 1 & 0 & 0 \\
2 & 2 & -1 & 0 & 1 & 0 \\
3 & -1 & 3 & 0 & 0 & 1
\end{matrix}
\end{align*}
Wir haben nun eine zulässige Basislösung.
Wähle nun $j = 2$. Dann ist
\begin{align*}
\theta_0 &= \min \left \{ \frac12, \frac33 \right \} = \frac{1}{2}\\
l &= 1
\end{align*}
\begin{align*}
\begin{matrix}
-6 & -3 & -3 & 0 & 0 & 0 \\
1 & 3 & \underline{2} & 1 & 0 & 0 \\
2 & 2 & -1 & 0 & 1 & 0 \\
3 & -1 & 3 & 0 & 0 & 1
\end{matrix}&\longrightarrow\begin{matrix}
\tfrac{-9}{2} & \tfrac{3}{2} & 0 & \tfrac{3}{2} & 0 & 0 \\
\tfrac{1}{2} & \tfrac{3}{2} & 1 & \tfrac{1}{2} & 0 & 0 \\
\tfrac{5}{2} & \tfrac{7}{2} & 0 & \tfrac{1}{2} & 1 & 0 \\
\tfrac{3}{2} & \tfrac{-11}{2} & 0 & \tfrac{-3}{2} & 0 & 1
\end{matrix}
\end{align*}
Die Lösung ist dann:
\begin{align*}
\overline{x} &= (0,\tfrac{1}{2},0,\tfrac{5}{2},\tfrac{3}{2})^T \\
c^T\overline{x} &= \tfrac{-9}{2}
\end{align*}
\end{example}

\begin{quote}
\textbf{Bemerkung: } beachte, dass Zykel auftreten können ( vgl. Übung )
\end{quote}
Standardregel zur Vermeidung von Zykeln sorgt dafür, dass die $0$-te Zeile lexikographisch wächst.
\begin{definition}
Ein Vektor $v \in \mathbb R^n$ ist \Begriff{lexikographisch} positiv (negativ), wenn die erste von Null verschiedene Komponente positiv (negativ) ist.
Wir schreiben $v >{}^L$ ($v <{}^L$).
Ein Vektor $v \in \mathbb R^n$ ist lexikographisch größer (kleiner) als $w \in \mathbb R^n$ gdw $v-w >{}^L 0$ ($v-w <{}^L 0$).
Es sei $V$ eine endliche Menge von Vektoren. Dann ist das lexikographische Minimum der lexikographisch kleinste Vektor aus $V$.
Wir schreiben $\LexMin\limits_{v \in V}(v)$.
\end{definition}
Bisher haben wir im Simplex-Algorithmus folgendes gemacht:
\begin{align*}
\min\limits_{i:x_{i,j} > 0} \left \lbrack \frac{x_{i,0}}{x_{i,j}} \right \rbrack
\end{align*}
dies ersetzen wir durch:
\begin{align*}
\LexMin_{i:x_{i,j} > 0} \left \lbrack \frac{x_i}{x_{i,j}} \right \rbrack
\end{align*}

\begin{theorem}
Es seien die Zeilen $x_1, \dotsc, x_m$ des Tableaus lexikographisch positiv. Dann sorgt die folgende Pivotregel dafür, dass die Zeilen lexikographisch positiv bleiben und dass die $0$te Zeile lexikographisch wächst und somit der Simplex-Algorithmus endlich ist.
\begin{enumerate}
\item Wähle eine beliebige Spalte mit $x_{0,j} < 0$
\item Wähle die Zeile $l$ mit
\begin{align*}
\frac{x_l}{x_{l,j}} &= \LexMin_{i:x_{i,j} > 0} \left \lbrack \frac{x_i}{x_{i,j}} \right \rbrack
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
Zunächst zeigen wir, dass die Zeilen lexikographisch positiv bleiben.
Die Pivotzeile $l$ wir zu
\begin{align*}
\hat{x}_l &= \frac{x_l}{x_{l,j}}
\end{align*}
Da $x_{l,j} > 0$ und $x_l >^L 0$ gilt $\hat{x}_l >^L 0$. Wenn $i \neq l$ und $x_{i,j} > 0$ ist, so gilt:
\begin{align*}
\hat{x}_i &= x_i - \frac{x_lx_{i,j}}{x_{l,j}} \\
&= \underbrace{x_{i,j}}_{>0} \underbrace{ \left \lbrack \frac{x_i}{x_{i,j}} - \frac{x_l}{x_{l,j}} \right \rbrack }_{>^L 0} >^L 0
\end{align*}
Wenn $i \neq l$ und $x_{i,j} \leq 0$ ist, so gilt:
\begin{align*}
\hat{x}_i &= x_i - \frac{x_{i,j} x_l}{x_{l,j}} \\
&= \underbrace{x_i}_{>0} + \frac{\overbrace{\lvert x_{i,j} \rvert}^{\geq 0} \overbrace{x_l}^{>^L 0}}{\underbrace{x_{l,j}}_{>0}} >^L 0
\end{align*}
Die $0$-te Zeile wird zu
\begin{align*}
\hat{x}_0 &= x_0 - \frac{x_{0,j} x_l}{x_{l,j}} \\
&= \underbrace{x_0}_{>0} + \frac{\overbrace{\lvert x_{0,j} \rvert}^{\geq 0} \overbrace{x_l}^{>^L 0}}{\underbrace{x_{l,j}}_{>0}} >^L x_0
\end{align*}
also wächst die $0$-te Zeile lexikographisch.
\end{proof}
\begin{quote}
\textbf{Bemerkung: }
\begin{enumerate}[a)]
\item die Auswahl der Pivotzeile bei gegebener Pivotspalte ist nun eindeutig.
\item die Vorraussetzung $x_i >^L 0, (i = 1,\dotsc,m)$ ist durch die Einheitsmatrix in den Spalten $1,\dotsc,m$ und durch eine zulässige positive Anfangslösung erfüllbar.
\end{enumerate}
\end{quote}

\begin{lemma}
Sei $\overline{c}^T = ( x_{0,1}, \dotsc , x_{0,n} )$ die relativen Kosten für ein Tableau $X_1$, das seine ($m \times m$ ) Einheitsmatrix enthält. Sei $x^0$ die zulässige Basislösung und $y$ eine beliebige Lösung von $Ay=b$ ( nicht notwendig positiv ). Dann gilt:
\begin{align*}
c^Ty &= c^Tx^0 + \overline{c}^Ty
\end{align*}
\end{lemma}
\begin{proof}
Wegen $\overline{c}^T = c^T - z^T = c^T - c^T_B B^{-1}A$ (siehe \eqref{eq:rel_kosten}), gilt:
\begin{align*}
\overline{c}^Ty &= c^Ty - c^T_B B^{-1}\underbrace{Ay}_{=b} \\
 &= c^Ty - c^T_B B^{-1}b \\
 &= c^Ty - c^T_B x_B^0 \\
 &= c^Ty - c^T x^0
\end{align*}
\end{proof}
\begin{theorem}\textbf{Variante von Blond}
Wenn wir im Simplex-Algorithmus die folgende Pivotregel benutzt, so endet dieses Verfahren nach endlich vielen Pivotschritten:
\begin{enumerate}
\item \label{blond_1}wähle die Spalte $j$ mit $j = \min\{h | \overline{c}_h = x_{0,h} < 0 \}$.
\item \label{blond_2}wähle die Zeile $l$ mit
\begin{align*}
B(l) &= \min \bigg \{ B(i) \bigg | x_{i,j} > 0 \text{ und } \frac{x_{i,0}}{x_{i,j}} \leq \frac{x_{k,0}}{x_{i,k}} \forall k, x_{k,j} > 0 \bigg \}
\end{align*}
\end{enumerate}
\end{theorem}
\begin{proof}
Wir finden also einen Widerspruch zu der Annahme, dass ein Zykel existiert.
Damit ein Zykel existiert, muss es eine endliche Folge von Pivotschritten geben, mit der man von einer zulässigen Basislösung wieder zurückkommen kann.
Während dessen bleiben die Kosten gleich und die $x_{l,0}$ assoziert mit jedem Pivotelement müssen $0$ sein.
Ansonsten ist $\theta_0 > 0$ und der Kostenwert würde sich ändern.

Wir lassen alle nicht beteiligten Zeilen und Spalten im Tableau weg und erhalten ein neues LP-Problem (mit $m$ Zeilen) , welches immernoch zykelt und wobei $x_{i,0} = 0$ für alle Zeilen $i = 1,\dotsc m$ gilt.

Es sei $q$ der größte Index einer Variablen, die während des Zyklus in die Basis aufgenommen wird.
Betrachte ein Tableau $T_1$, bevor die Variable $x_{q}$ in die Basis aufgenommen wird, und das Tableau $T_2$, wenn $x_q$ die Basis verlässt.
Es sei $p$ die Spalte, die in $T_2$ gewählt wird.
\begin{align*}
T_1 &=\begin{matrix}
& & & & & q \\
-z & &  &  &  &  < 0 \\
0 & &  &  &  &  \\
\vdots &  &  &  &  & \vdots \\
0 & &  &  &  &
\end{matrix}
\end{align*}
\begin{align*}
T_2 = &\begin{matrix}
 & & & & p & & q \\
 &-z &  &  & < 0 &  & 0 \\
 & 0 &  &  & < 0 &  & 0 \\
 & 0 &  & & < 0 &  & \vdots \\
l & 0 &  &  & \hat{x}_{l,p}>0 &  & 1 \\
 & 0 &  &  & < 0 &  & \vdots \\
 & 0 &  &  & < 0 &  & 0
\end{matrix}
\end{align*}
Wir wenden das obige Lemma an und wählne in $T_1$ als Basislösung $x^0$ und identisch $x_1$ mit $T_1$.
Aus $T_2$ definieren wir ein Lösung $y$ durch
\begin{align*}
y_j &= \begin{cases}
1 & j = p \\
-\hat{x}_{i,p} & A_j = \hat{B} \text{ mit } \hat{x}_{i,j} = 1 \\
0 & \text{sonst}
\end{cases}
\end{align*}
Dann ist $y$ zwar kein Basislösung ( da es eine Nicht-Nullkomponente zu viel hat ), aber eine Lösung von $T_2y = \vec{0}$ udn damit von $Ay = b$.
Also sind die Vorraussetzungen vom obigen Lemma erfüllt.
Die Kosten von $y$ sind ( bezogen auf $T_2$ ):
\begin{align*}
c^Ty &= \underbrace{c^Tx^*}_{=z} + \overline{\hat{c}}^T y\\
\overline{\hat{c}} &= (\dotsc, 0 \dotsc, \hat{x}_{0,p}, \dotsc, * , \dotsc) \begin{pmatrix}
\vdots \\
-\hat{x}_{i,p} \\
\vdots \\
1 \\
\vdots \\
0 \\
\vdots
\end{pmatrix} \\
&= z + \hat{x}_{0,p}
\end{align*}
Aus dem Lemma folgt dann:
\begin{align*}
c^Ty = c^Tx^0 + \overline{c}^Ty &&\text{bezogen auf $T_1$}
\end{align*}
Da die Kosten immer gleich bleiben, gilt $c^Tx^0 = z$. Es folgt:
\begin{align}
\overline{c}^Ty = \hat{x}_{0,p} < 0 &&\text{weil $p$ gewählt wird}
\label{eq:rel_kost_le_0}
\end{align}
Wegen der Wahl der Pivotspalte in $T_1$ gilt:
\begin{align*}
\overline{c}_j & \begin{cases}
\geq 0 & j < q \\
< 0 & j = q
\end{cases}
\end{align*}
und wegen der Wahl der Pivotzeile in $T_2$ ( $q=B(l)$ ) ( wenn nicht würden wir eine andere Zeile auswählen ):
\begin{align*}
y_j &= \begin{cases}
-\hat{x}_{l,p} < 0 & j=q \\
0, 1, -\hat{x}_{i,p} \geq 0 & j<q
\end{cases}\Vert
\end{align*}
Deshalb gilt:
\begin{align}
\overline{c}^T y &= \sum_{j < q} \underbrace{\overline{c}_j}_{\geq 0} \underbrace{y_j}_{\geq 0} + \overline{c}_q y_q \nonumber \\
&\geq \underbrace{\overline{c}_q}_{<0} \underbrace{y_q}_{<0} > 0
\end{align}
Das ist ein Widerspruch zu \eqref{eq:rel_kost_le_0}
\end{proof}

\subsection{Bestimmung der Startlösung}
\emph{einfach:} $Ax \leq b$ wird zu $Ax + Iy = b$ \newline
\emph{ansonsten:} füge neue Variablen $y_1,\dotsc, y_m$ in den linknen Teil des Tableaus und wende die 2-Phasen Methode an.
% hier matrix mit links b dann I dann A
Startlösung für Phase 1: $y_i = b_i, i=1,\dotsc,m, x_j =0 j=1,\dotsc,n$.\newline
\textbf{In Phase 1 minimieren wir die Kostenfunktion $\xi = \sum_{i=1}^m y_i$.}\newline
Es gibt 3 mögliche Ergebnisse des Simplex-Verfahrens:
\begin{enumerate}[Fall 1)]
\item $\xi =0$ und alle $y_i$ sind außerhalb des Bereichs.
In diesem Fall haben wir eine zulässige Basislösung für das Originalproblem.
\item $\xi > 0$
In diesem Fall gibt es keine zulässige Lösung des Problems.
\item $\xi = 0$, aber einige der zusätzlichen Variablen sind noch in der Basis mit Wert $y_k = 0$.
Angnommen die $l$-te Zeile ist der Variablen $y_k$ zugeordnet. D.h. $k = B(l)$ und $x_{l,0} = 0$.
Dann können wir mit einem Element $x_{l,j} \neq 0$ (nicht notwendig positiv) der Spalte $j$, die der Variablen
$x_{j-m}$ zugeordnet ist, pivotisieren.
Da $\theta_0 = 0$ ist, gibt es keinen Verlust der Zulässigkeit und keine Änderung der Kosten $\xi$.
Im Allgemeinen ist dies kein Pivotisieren, da das Pivotelement hier einen negativen Wert haben kann oder
die Kosten größer $0$ sein dürfen, sondern ein Entfernen der zusätzlichen Variablen.\newline
\textbf{Sonderfall:} wir haben eine Zeile mit Nullen in allen Spalten, die den Originalvariablen zugeordnet sind.
Das bedeutet, dass $A$ nicht vollen Rang hatte. Folglich kann man diese Zeile einfach löschen und in Phase 2 mit
niedrigerer Dimension weitermachen.
\end{enumerate}
In Phase 2 streicht man die neuen Variablen $y_i$ und löst das Problem mit der Originalzielfunktion $c^Tx$.
\newline
\textbf{Phase 2}\newline
\begin{verbatim}
procedure two-phase;
begin
	# Phase 1
	unzulässig = falsch;
	redundant = falsch;
	füge neue Variablen $\y_1m, \dotsc, y_m$ ein;
	rufe Simplex mit Kosten $\xi = \sum_{i=1}^m y_i auf
	# Phase 2
	if $\xi_{opt} > 0$ then unzulässig = true
	else begin
		entferne zusätzliche Variablen aus der Basis; (wenn $x_l \neq 0$)
		solange es eine Variable $x_i$ in der Basis gitb, die nicht entfern werden kann
			redundant = wahr
			lasse die zugeordnete Nullzeile weg
		end
		streiche die $y_j$ Variablen aus dem Tableau
		rufe Simplex mit Originalkosten $c^Tx$ auf.
	end
end	
\end{verbatim}

\begin{example}
\begin{align*}
\min(-x_1 + 3x_2 ) \\
x_1 - x_2 + x_3 &= 4 \\
2x_1 + x_2 + x_4 &= 14 \\
-x_1 + x_2 + x_5 &= 8 \\
x_1 + 2x_2 -x_6 = 4 \\
x_1 + 2x_2 = 10
\end{align*}
\begin{align*}
\begin{matrix}
	& x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & y_1 & y_2\\
-14	& -2 & -4 & 0 & 0 & 0 & 1 & 1 0 & 1 0 \\
4 	& 1 & -1 & 1 \\
14	& 2 & 1 & & 1 \\
8	& -1 & 1 & & & 1\\
4	& 1 & \underline{2} & & & &-1 & 1\\
10	& 1 & 2 & & & & & &1
\end{matrix}\\
\begin{matrix}
-6	& x_1 & x_2 & x_3 & x_4 & x_5 & x_6 & y_1 & y_2\\
-6	& 0 & 0 & 0 & 0 & 0 & 1 & -1 & 2 \\
6 	& \tfrac{3}{2} & 0 & 1 & & & -\tfrac{1}{2} & \tfrac{1}{2}\\
12	& \tfrac{3}{2} & 0 & & 1 & & \tfrac{1}{2} & -\tfrac{1}{2}\\
6	& -\tfrac{3}{2} & 0 & & & 1 & \tfrac{1}{2} & -\tfrac{1}{2}\\
2	& \tfrac{1}{2} & 1 & & & & -\tfrac{1}{2} & \tfrac{1}{2}\\
6	& 0 & 0 & & & & \underline{1} & -1 &1
\end{matrix}\\
\begin{matrix}
-15	& \tfrac{-5}{2} & 0 & 0 & 0 & 0 & 0 & 0 & \tfrac{-3}{2}\\
0	& 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
9 	& \tfrac{3}{2} & 0 & 1 & & & & & \tfrac{1}{2}\\
9	& \tfrac{3}{2} & 0 & & 1 & & & & -\tfrac{1}{2}\\
3	& -\tfrac{3}{2} & 0 & & & 1 & & & -\tfrac{1}{2}\\
5	& \tfrac{1}{2} & 1 & & & & & & \tfrac{1}{2}\\
6	& 0 & 0 & & & & 1  & -1 &1
\end{matrix}\\
\begin{matrix}
0	& 0 & 0 & \tfrac{5}{3} & 0 & 0 & 0\\
6 	& 1 & 0 & \tfrac{2}{3} & & & \\
0	& 0 & 0 & -1 & 1 & & \\
12	& 0 & 0 & 1 & & 1 & \\
2	& 0 & 1 & \tfrac{-1}{3} & & & \\
6	& 0 & 0 & & & & 1
\end{matrix}\\
\overline{x} &= (6,2,0,0,12,6) \\
z_{opt} &= 0
\end{align*}
\end{example}

\subsection{Geometrische Interpretation des Simplex Verfahrens}
\begin{definition}
Als die \Begriff{affine Hülle} bezeichnet man die Menge aller Punkte die sich darstellen lassen mit 
\begin{align*}
\aff A &= \{ x | \sum_{a \in A} \lambda(a) a = x, \sum_{a \in A} \lambda(a) = 1\}
\end{align*}
Der Unterschied zur konvexen Hülle besteht darin, dass $\lambda$ auch negativ und größer als $1$ sein darf.
\end{definition}
\begin{definition} \Begriff{benachbart}:
\begin{enumerate}[(a)]
\item Zwei Ecken $\hat{x}$ und $\hat{y}$ eines Polytops $P$ heißen benachbart, wenn $\conv(\{\hat{x},\hat{y})$ eine Kannte von $P$ ist.
\item Zwei verschiedene zulässige Basislösungen $x,y$ eines LP heißen benachbart, wenn es Basen $B_x, B_y$ gibt mit
\begin{align*}
B_y &= ( B_x - \{ A_j \} ) \cup \{ A_k \} ( j \neq k ) && \text{und} \\
x &= B^{-1}_x b \\
y &= B^{-1}_y b
\end{align*}
\begin{quote}
\textbf{Bemerkung:} Zwei zulässige Basislösungen $x,y$ eines LP mit $c^Tx < c^Ty$ sind genau dann benachbart, wenn es einen zulässigen Simplexschritt gibt, durch den $x$ aus $y$ hevorgeht.
\end{quote}
\end{enumerate}
\end{definition}
\begin{theorem}
Es sei $P$ ein Polytop, $F = \{ x | Ax=b, x\geq 0 \}$ die zugehörige Menge der zulässigen Punkte des LP,
$\hat{x} = (x_1, \dotsc, x_{n-m})^T$ und $\hat{y} = (y_1, \dotsc, y_{n-m})^T$ verschiedene Ecken von $P$ und $x,y$ die zugeordnent Basislösungen. Dann sind folgende Aussagen äquivalent:
\begin{enumerate}[(a)]
\item $\hat{x},\hat{y}$ sind benachbart.
\item Für $\hat{z} \in \conv(\{\hat{x},\hat{y}\})$ mit $\hat{z} = \lambda \hat{z}^1 + (1-\lambda) \hat{z}^2$, $0 < \lambda < 1$ und $\hat{z}^1, \hat{z}^2 \in P$ dann sind $\hat{z}^1, \hat{z}^2 \in \conv(\{\hat{x},\hat{y}\})$. \label{th:eq_b}
\item $x$ und $y$ sind benachbart.
\end{enumerate}
\end{theorem}
\begin{proof}
\begin{description}
\item[(a) $\Rightarrow$ (b):] siehe Übung
\item[(b) $\Rightarrow$ (c):] Wir nehmen an, dass die zulässigen Basislösugnen $x, y $ aus $F$ nicht benachbart sind, aber zu Punkten $\hat{x},\hat{y}$ in $P$ gehören, die die Eigenschaft (\ref{th:eq_b}) besitzen.
Es seien $M_x, M_y$ die Menge der Spalten, di e zu den positiven Komponenten von $x$ und $y$ gehören.
Gäbe es keine zulässige Basislösung $w$ mit der Eigenschaft $w \neq x,y$ und $M_w \subset M_x \cup M_y$, so wäre
$y$ das eindeutige Optimums des LP's $Ax = b, x \geq 0$ mit dem Kostenvektor
\begin{align*}
c_j &= \begin{cases}
0 & A_j \in M_y\\
1 & A_j \in M_x \setminus M_y\\
G & \text{sonst}
\end{cases}
\end{align*}
wobei $G$ hinreichend groß gewählt ist.
Jede von $x,y$ verschiedene Basislösung $w$ (mit $M_w \nsubseteq M_x \cup M_y$) hat höhere Kosten als $x$.
Der Simplex-Algorithmus könnte also ausgehend von $x$ niemals das Optimum erreichen.
Diese Widerspruch zeigt, dass es eine zulässige Basislösung $w$ mit $w\neq x,y$ und $M_w \subset M_x \cup M_y$ geben muss.\newline
$\hat{w}$ liegt nicht in $\conv\{\hat{x},\hat{y}\}$, da $\hat{x},\hat{y},\hat{w}$ verschiedene Ecken von $P$ sind und auch nicht in $\aff\{\hat{x},\hat{y}\}$.
Sei $z = \frac{1}{2}(x+y)$ und $d = z-w$. $d$ hat außer für Spalten aus $M_x \cup M_y$ nur verschwindende Komponenten ($= 0$). Da gilt
\begin{align*}
Ax = b, x \geq 0 \and Ay = b, y \geq 0 &\Rightarrow Az=b, z \geq 0 \\
Aw = b, w \geq 0 &\Rightarrow Ad = 0
\end{align*}
Es gibt also ein $\theta > 0$ mit $u_1 = z + \theta d, u_2 = z - \theta d$ zulässige Lösungen.
Folglich ist $z = \frac{1}{2}( u_1 + u_2 )$ mit $\hat{u}_1, \hat{u}_2 \notin \conv\{\hat{x},\hat{y}\}$ im Widerspruch zu (\ref{th:eq_b}).
\item[(c) $\Rightarrow$ (a):] Es seien $B_x, B_y$ die Basen zu $x,y$ mit $B_y = B_x \cup \{A_j\} \setminus \{A_k\}$ und Spalten $A_j, A_k, j \neq k$.
Sei
\begin{align*}
c_i &= \begin{cases}
0 & \text{für } A_i \in B_x \cup B_y\\
1 & \text{sonst}
\end{cases}
\end{align*} 
Alle zulässigen Lösungen, die Konvexkombinationen von $x$ und $y$ sind, sind optimale Lösungen von $\min c^Tw, Aw=b, w\geq 0$. Umgekehrt sind dies auch die einzigen optimalen Lösungen.
Wenn nämlich $z$ optimal ist, so ist $z$ eine Konvexkombination von optimalen Basislösungen und insbesondere hier von Basislösungen mit Basen, die Teilmenge von $B_x \cup B_y$ sind.
Beachte, dass $x$ und $y$ die einzigen solchen Basislösungen sind!
Folglich ist $z$ Konvexkombination von $x$ und $y$.
Es folgt, dass nur Konvexkombinationen $w$ von $x$ und $y$ die Bedingungen $Aw =b , w\geq 0, c^Tw \leq c^Tx = 0$ erfüllen.
Es folgt, dass in $P$ nur die Punkte $\hat{w} \in \conv\{\hat{x},\hat{y}\}$ die Bedingung $d^T\hat{w} \leq d^T\hat{x}$ erfüllen, wobei $d$ wie im Beweis 1.10. \ref{th:basisloesung_eq_ecke} ( Basislösungen $\Leftrightarrow$ Ecken ) definiert ist.
Durch $d$ wird eine Hyperebene an $P$ definiert, die genau $\conv\{\hat{x},\hat{y}\}$ ausschneidet.
Also ist $\conv\{\hat{x},\hat{y}\}$ eine Kante und $\hat{x},\hat{y}$ sind benachbart.
\end{description}
\end{proof}

\section{Duale LPs}

Um hierrauf das Optimalitätskriterium von Satz 4.5 anwenden zu können, müssen wir es in Satadardform schreiben. Durch Einfühtrung von Überschussvariablen $y_i \in Z \setminus M $ und von positiven Variablen $x^+_j, x^-_j$ für $j \in S \setminus N ( x_j = x^+_j - x^-_j)$ erhalten wir das folgende äquivalente LP-Problem:
\begin{align*}
\min \hat{c}^T\hat{x} & \hat{A}\hat{x} = \hat{b}, \hat{x} \geq 0 \\
\hat{c} &= ( c_1, \dotsc, c_{n'}, c_{n'+1}, -c_{n'+1} , \dotsc , c_n, -c_n, 0, \dotsc ,0) \\
\hat{x} &= (x_1, \dotsc , x_{n'}, x^+_{n'+1}, x^-_{n'+1}, \dotsc , x^+_n, x^-_n, y_{m'+1}, \dotsc, y_n )\\
\hat{A} &= \left ( \begin{matrix}
| &  & | & | & | & & | & |  \\
A_1 & \hdots & A_{n'} & A_{n'+1} & -A_{n'+1} & \hdots & A_n & -A_n \\
| &  & | & | & | & & | & | 
\end{matrix} \binom{0}{-I} \right )
\end{align*}
Wegen des Optimalitätskriteriums $\overline{c} \geq 0$ und wegen dem Simplex-Algorithmus gilt:
wenn $\hat{x}_0$ ein optimale Lösung des obigen LPs ist, so gibt es eine Basis für das LP mit
\begin{align*}
\hat{c}^T - (\underbrace{\hat{c}_B^T B^{-1}}_{\Pi^T} )\hat{A} &\geq 0
\end{align*}
Daher ist $\Pi^T$ eine zulässige Lösung der Ungleichungen $\Pi^T \hat{A} \leq \hat{c}^T$ mit $\Pi^T \in \mathbb R^m$.
Diese Ungleichungen können wegen der Darstellung des LP-Problem in drei Gruppen aufgeteilt werden:
\begin{enumerate}[(a)]
\item \emph{$\Pi^TA_j \leq c_j$ für alle $j \in N$}
\item \emph{$\Pi^TA_j \leq c_j$ und $\Pi^T(-A_j) \leq (-c_j)$ für alle $j \in S\setminus N$}, also $\Pi^TA_j = c_j$ für alle $j\in S\setminus N$.
\item \emph{$-\Pi_i \leq 0$ für alle $i \in Z\setminus M$}
\end{enumerate}
Diese Bedingungen definieren dei Ungleichungen eines neuen LP-Problems, des sogenannten Dualen LP-Problems.
Das erste LP-Problem wird primales LP-Problem genannt.
Wenn wir die Kostenfunktion $\max \Pi^T b$ wählen so ist der Vektor $\Pi^T = \hat{c}^T_B B^{-1}$ nicht nur zulässig
sondern, wie wir zeigen werden, auch optimal.

Es seien $x$ und $\Pi$ zulässige Lösungen des primalen bzw. dualen LP-Problems. So gilt:
\begin{align*}
c^Tx &\geq \Pi^T A x && \text{da $c^T \geq \Pi^TA$}\\
&\geq \Pi^T b && \text{da $Ax \geq b$} \tag{Primal dom. Dual}\label{eq:ctx_geq_pitb}
\end{align*}
Die Kosten einer primalen Lösung dominieren also immer die Kosten einer dualen Lösung.
Wenn das primale Problem eine zulässige Lösung besitzt, so kann das duale Problem nicht kostenmäßig unbeschränkt sein.
Es sei also $\hat{x}^0$ eine zulässige optimale Lösung des primalen Problems.
Dann hat das duale Problem die zulässige Lösung $\Pi^0$ mit ${\Pi^0}^T = \hat{c}^T_B B^{-1}$ und den Kosten von 
${\Pi^0}^T b = \hat{c}^T_B B^{-1} b = \hat{c}^T \hat{x}^0$.
Die Kosten der dualen Lösung sind also gleich den Kosten der primalen Lösung. Wegen $c^Tx \geq \Pi^T b$ ist daher $\Pi^0$ optimal.

\begin{theorem}
Ein Paar $x,\Pi$ ist ein zulässiges optimales Paar von Lösungen für ein primale-duales Paar von LP-Problemen genau dann wenn:
\begin{align*}
u_i = \Pi_i ( a_i^T x - b_i ) = 0 \forall 1 \leq i \leq m \\
v_j = ( c_j - \Pi^T A_j ) x_j = 0 \forall 1 \leq j \leq n
\end{align*}
\end{theorem}
\begin{proof}
Aus Definition 81 folgst, dass $n \geq 0 $ alle $i$  un d $v_j \geq 0$ für alle $j$ . Definiere $u = \sum_{i=1}^m u_i \geq 0$ und $v = \sum_{j=1}^n v_j \geq 0$.
Dann gilt $u=0$ und $v=0$ wenn alle $m$ und $n$ die Bedingungen erfüllen.
\begin{align*}
u + v &= c^T x - \Pi^T b
\end{align*}
da die gemischten Terme sich gegenseitig wegheben. Also sind $m$ und $n$ Bedingungen erfüllt, genau dann wenn $u +v = 0$ bzw. $c^Tx = \Pi^T b$ ist.
Dies ist jedoch notwending und hinreichend, wegen \eqref{eq:ctx_geq_pitb}, dafür dass $x$ und $\Pi$ optimal sind.
\end{proof}

\section{Laufzeit des Simplex}

\begin{proof}
Zu Klee-Minty:
Per Induktion nach $d$. Für $d=1$ gibt es zwei Basislösungen:
\begin{align*}
(x_1,r_1, s_1) &\in \{ (0,\epsilon, 1-\epsilon) , (1,1-\epsilon,0) \}
\end{align*}
mit unterschiedlicher Komponente $x_1$ und sie sind adjazent.
\emph{Vorraussetzung:} Die Aussage gilt für $d$-dimensionale Hyperwürfel und sei $S_1 ,\dotsc, S_{2^d}$ die entsprechende Ordnung der Teilmengen von $\{1,\dotsc, d\}$.
Diese Teilmengen sind auch Teilmengen von $\{1, \dotsc, d+1\}$ und $x^{S_j}_{d+1} = \epsilon x^{S_j}_{d}$ (weil $r_{d+1} = 0$ ).
Nach der Induktionsvorraussetzung gilt $x^{S_1}_d < \dotsc , x^{S_{2^d}}_{d}$, also $x^{S_1}_{d+1} < \dotsc , x^{S_{2^d}}_{d+1}$
Nun betrachte die übrigen Teilmengen von $\{1,\dotsc, d+1\}: S'_j = S_j \cup \{d+1\}$ für $j=1,\dotsc,2^d$.
Nach Lemma b gilt wegen $d+1 \in S'_j$ und $d+1 \notin S_{2^d}$.
$x^{S'_j}_{d+1} > x^{S_{2^d}}_{d+1}$ und $x^{S'_j}_{d+1} = 1 - x^{S_j}_{d+1}$ also insgesamt: 
\begin{align*}
x^{S_1}_{d+1} < \dotsc < x^{S_{2^d}}_{d+1} < x^{S'_{2^d}}_{d+1} < \dotsc < x^{S'_1}_{d+1}
\end{align*}
Nach der Induktionsvorraussetzung sind $x^{S_j}$ und $x^{S_{j+1}}$ benachbart und analog $x^{S'_j}$ und $x^{S'_{j+1}}$.
Zusätzlich sind auch $x^{S_{2^d}}$ und $x^{S'_{2^d}}$ benachbart (ersetze in der Basis zu $S_{2^d}$ die Spalte zu $s_{d+1}$ durch die Spalte zu $r_{d+1}$ ).
\end{proof}

\section{Elipsoid-Algorithmus}

Wir betrachen nun LI ( linear equalities ): zu einer gegebnen Matrix $A \in \mathbb Z^(m\times n)$ und $b \in \mathbb Z^(m)$ entscheide, ob es einen zulässigen Vektor $x$ so dass $Ax \leq b$ gilt.
\begin{theorem}
Für LI existiert genau dann ein Polynomialzeitalgorithmus, wenn auch einer für LP exisiert.
\end{theorem}
\begin{proof}
\begin{description}
\item\textbf{LI zu LP:} Trivial. Ergänze ein Kostenfunktion ( z.B. $c = \vec{0}$ ).
\item\textbf{LP zu LI:} Gegeben sei Algorithmus $\mathcal A$ der LI in Polynomialzeit löst. Wir beschreiben einen Polynomialzeitalgorithmus für LPs, der $\mathcal A$ verwendet:
\begin{enumerate}
	\item Teste, ob $LP$ zulässig  ist ( mit $Ax \geq b, Ax \leq b, x \geq 0$ ). Wenn $\mathcal A$ antwortet "NEIN" dann ist das LP unzulässig.
	\item Teste die Zulässigkeit der Ungleichungen $Ax \geq b, Ax\leq b, x \geq 0, c^Tx \leq -2^{2L} -1$. Für Basislösungen gilt:
\begin{align*}
 \lvert c^Tx\rvert &= \left \lvert \sum c_j x_j \right \rvert \\
	&\leq \underbrace{\max_j x_j}_{\leq 2^L} \cdot \underbrace{\sum \lvert c_j \rvert}_{\leq 2^L} \leq 2^{2L}
\end{align*}
		Wenn $\mathcal A$ antwortet "JA", dann existieren zulässige Lösungen deren Kosten kleiner sind, als die Kosten aller Basislösungen. Folglich ist $\{ c^Tx | x \text{ zulässig} \}$ nicht nach unten beschränkt.
	\item Zunächst bestimme $K \in \mathbb Z$ mit $-2^{4L} \leq K \leq 2^{4L}$ und $K 2^{-2L} < c^T\hat{x} \leq (K+1) 2^{-2L}$.
Dieses $K$ können wir durch binäre Suche mit höchstens $4L +2$ aufrufen von $\mathcal A$ bestimmen:
\begin{align*}
	Ax \geq b, Ax \leq b, x\geq 0, 2^{2L}c^Tx \leq a \text{ für gewissen Zahlen }a
\end{align*}
	\item Bestimme die Basis von $\hat{x}$. Teste ob die folgenden Ungleichungen ( für $k = 1,\dotsc,m$ ) erfüllt sind:
\begin{align*}
Ax \leq b, Ax \geq b , K \leq 2^{2L} c^Tx \leq K+1, x \geq 0, x_k \leq 0 \\
x_j \leq 0 \forall j \in S(k)
\end{align*}
Wobei die Menge $S(k)$ die Mengen der Indices $< k$ ist, für die die Anwort "JA" war.
\end{enumerate}
\end{description}
\end{proof}
\begin{quote}
\textbf{Bemerkung:}
\begin{enumerate}[(a)]
 \item Jede Menge von $m$ Spalten, dier nicht in $S(n+1)$ sind, können für eine Basis von $\hat{x}$ gewählt werden.
 \item Es gibt mindestens $m$ Spalten, die nicht in $S(n+1)$ sind, falls keine Entartung auftritt.
 \item Nach Bestimmung der Basis berechne $\hat{x} = B^{-1}b$.
\end{enumerate}
\textbf{Frage:} Was passiert bei Entartungen?
\end{quote}
\begin{lemma}
Das System der linearen Ungleichungen
\begin{align*}
a_i^Tx &\leq b_i \label{eq:li}\tag{LI}
\end{align*}
hat genau dann eine Lösugen wenn das System der linear strikten Ungleichungen
\begin{align*}
a_i^Tx &< b_i + \epsilon  \label{eq:lsi}\tag{LSI}
\end{align*}
mit $\epsilon = 2^{-2L}$.
\end{lemma}
\begin{proof}
\textbf{LI zu LSI:} Trivial, da jede Lösung des LI auch eine Lösung des LSI ist.\newline
\textbf{LSI zu LI:}
Sei $x_0$ Lösung von \eqref{eq:lsi}. Sei $I = \{ a_i :  b_i \leq a_i^Tx_0 < b_i + \epsilon \}$.\newline
\emph{Annahme: $\exists a_j$ unabhängig zu allen $a_i$ in $I$.}\newline
Das Gleichungssystem $a_i^T z = 0 \forall a_i \in I, a_j^Tz = 1$ hat eine Lösung $z_0$. Setze $x_i = x_0 + \lambda z_0$ ( mit $\lambda >0$ geeignet gewählt ).
\begin{align*}
\forall i \in I: &a_i^T x_i = a_i^T( x_0 + \lambda z_0) = a_i^Tx_0\\
&a_j^Tx_i = a_j^T(x_0 + \lambda z_0) = a_j^Tx_0 + \lambda \\
\forall k \notin I: & a_k^Tx_k = a_k^T(x_0 + \lambda z_0) = a_k^Tx_0 + \lambda a_k^T z_0 < b_k + \lambda a_k^T z_0
\end{align*}
Durch geeignete Wahl von $\lambda$ können wir die Menge $I$ vergrößern ( mindestens eine Zeile ). Nach $m$ Schritten muss solch ein Verfahren aber abbrechen und wir erhalten:
\begin{align*}
\forall j : & a_j 0 \sum_{a_i \in I'} \beta_{j,i} a_i
\end{align*}
für geeignete Zahlen $\beta_{j,i}$ und $I' \subseteq I$, da $I'$ linear unabhängige Menge von Zeilenvektoren.
Nach der Cramerschen Regel folgt: $\beta_{j,i} = \frac{D_{j.i}}{\lvert D \rvert}$ mit $D_{j,i} \in \mathbb Z$ und $D_{j,i}, D \leq 2^{L}$
Betrachte Lösung $\hat{x}$ des Gleichungssystems $a_i^Tx=b_i$ mit $a_i \in I'$ ( es exisitert eine Lösung, da $I'$ linear unabhängige Zeilen enthält ).
\newline\newline
Zeige: $\hat{x}$ ist Lösung von \eqref{eq:li}.
\begin{align*}
\forall j: \lvert D \rvert ( a_j^T \hat{x} - b_j ) &= \sum_{a_i \in I'} \lvert D \rvert \frac{D_{j,i}}{\lvert D \rvert} a_i^T\hat{x} - \lvert D \rvert b_j \\
&= \sum_{a_i \in I'} D_{j,i} b_i - \lvert D \rvert b_j \\
&= \sum_{a_i \in I'} D_{j,i} b_i - \lvert D \rvert b_j + \underbrace{\lvert D \rvert a_j^Tx_0}_{=\sum_{a_i\in I'} D_{j,i}a_i^Tx_0 } - \lvert D \rvert a_j^Tx_0 \\
&= -\sum_{a_i \in I'} D_{j,i} (a_i^T x_0 - b_i ) + \lvert D \rvert \underbrace{( a_j^Tx_0 - b_j)}_{< \epsilon} \\
&< \epsilon \sum_{a_i \in I'} \lvert D_{j,i} + \epsilon \lvert D \rvert \leq \frac{1}{2^{2L}} \underbrace{(m+1)}_{<2^L} 2^L < 1
\end{align*}
Es folgt:
\begin{align*}
\hat{x}_i &= \frac{\alpha_i}{\beta_i} && \text{mit } \beta_i | \lvert D \rvert \\
\Rightarrow \lvert D \rvert (a_j^T\hat{x}-b_j) \in \mathbb Z \\
\Rightarrow a_j^T\hat{x} - b_j \leq 0 \forall 0
\end{align*}
\end{proof}
\begin{lemma}
Wenn ein LSI-System der Eingabegröße $L$ eine Lösung hat, dann hat die Menge der Lösungen innerhalb der Kugel $\{x:\Vert x \Vert \leq n2^L \}$ ein Volumen $\geq 2^{-(n+2)L}$.
\end{lemma}
\begin{proof}
Wenn $Ax < b$ eine Lösung hat, dann hat auch $Ax<b, -2^L< x_i < 2^L, i = 1,\cdots,n$ eine Lösung.
Das Polytop $Ax \leq b, -2^L \leq x_i \leq 2^L$ einen inneren Punkt.
Es existieren folglich $n+1$ affin unabhängige Ecken $\{v_0,\dotsc, v_n\}$ in dem Polytop.
Aus der Konvxität folgt, dass alle inneren Punkte der konvexen Hülle von $\{v_0,\dotsc, v_n\}$ auch automtatisch Lösungen von $Ax < b$ innerhalb der Kugel $\Vert x \Vert \leq n2^L$ sind.
Das Volumen dieser konvexen Hülle ( in diesem Fall ein Simplex ) ist
\begin{align*}
\frac{1}{n!} \left \lvert \det \begin{pmatrix}
1 & 1 & \hdots &1 \\
v_0 & v_1 & \hdots & v_n 
\end{pmatrix} \right \rvert &\neq 0
\end{align*}
Wir wissen, jede Ecke kann man schreiben als
\begin{align*}
v_i &= \frac{u_i}{D_i} && \text{mit } u_i \in \mathbb Z^n, \lvert D_i \rvert \leq 2^L
\end{align*}
Daher hat die Determinate der Matrix im Nenner $\prod \lvert D_i \rvert$.
Das Volumen ist folglich 
\begin{align*}
\geq (n! \prod_{i=0}^n \lvert D_i \rvert )^{-1} &\geq \frac{1}{2^L}\frac{1}{2^L}^{n+1}
\end{align*}
\end{proof}
\begin{theorem}
Der Elipsoid-Algorithmus entscheidet korrekt, ob das LSI-System eine Lösung hat.
\end{theorem}
\begin{proof}
Wenn der Algorithmus einen zulässigen Punkt $t_k$ bei Schritt 2 liefert, dann ist er korrekt\newline
\emph{Annahme: Der Algorithmus liefert "NEIN" und es gibt trotzdem eine Lösung}\newline
Wegen dem obigen Lemma, gibt es eine Menge $S$ von Lösungen innerhalb der Kugel $E_0 = \{x\in \mathbb R^{-1} | x^TB_0^{-1} x\leq 1 \}$ mit Volumen $\geq 2^{-(n+2)L}$.
Folglich ist $E_0 = \{x\in \mathbb R^{-1} | x^Tx\leq n^22^{2L} \} = \{x\in \mathbb R^{-1} | \Vert x \Vert \leq n2^{L} \}$.
Wegen dem Satz ist $S \subset E_j , j = 0, \dotsc , K$ und es gilt 
\begin{align*}
\vol(E_k) &< \vol(E_0) \cdot 2^{-\frac{K}{2(n+1)}} \\
	&= \vol( E_0 ) \cdot 2^{-16n(n+1)L]{2(n+1)}} \\
	&< \underbrace{(2n^22^{2L})^n}_{\text{Würfel}} \cdot 2^{-8nL} \\
	&< 2^{-(n+2)L}
\end{align*}
Dies ist aber ein Widerspruch zum Lemma.
\end{proof}

