\chapter{Lineare Programme}
\section{Einleitung}
\begin{definition}
Seien 
\begin{itemize}
\item $A$ eine ganzzahlige $m\times n$-Matrix mit den Zeilenvektoren $a_i^T$
\item $\{M_1,M_2\}$ eine Partition von $\laue{m}$
\item $\{N_1,N_2\}$ eine Partition von $\laue{n}$
\item $x \in \mathbb R^n, b,c \in \mathbb Z^n$
\end{itemize}
Dann heißt das Problem $\min(c^Tx = \sum_{i=1}^m c_ix)$ mit den Randbedingungen
\begin{align*}
a_i^Tx &= b_i \; &\forall i \in M_1 \\
a_i^Tx &\geq b_i \; &\forall i \in M_2 \\
x_j &\geq 0 \; &\forall j \in N_1 \\
x_j &\gtrless 0 \; &\forall j \in N_2 \\
\end{align*}
\Begriff{allgemeines Lineares Programm} oder kurz allgemeines LP-Problem.\newline %
Für $M_1 = N_2 = \emptyset$ ist es in \Begriffx{kanonische Form}{kanonischer Form}.\newline %
Für $M_2 = N_2 = \emptyset$ ist es in \Begriff{Standardform}.
\end{definition}

\begin{theorem}
Alle drei Formen des LP-Problems sind äquivalent ( d.h. können ineinander überführt werden ).
\end{theorem}
\begin{proof}
Da LP-Probleme in kanonischer und Standardform automatisch auch allgemeine LP-Probleme sind, reicht es zu zeigen, dass jedes allgemeine LP-Problem in ein Problem in kanonischer und Standardform überführt werden kann.
\begin{enumerate}
\item{\textit{allgemeine in kanonische Form}} \newline
Die Umwandlung erfolgt durch Ersetzen von Bedingungen durch äquivalente Bedingungen. Dabei muss das Gleichungssystem (Matrix) um zusätzliche Gleichungen (Zeilen) und Variablen (Spalten) erweitert werden.
Alle Randbedingungen der Form $a_i^Tx = b_i$ sind äquivalent zu $a_i^Tx \geq b_i \land -a_i^Tx \geq -b_i$. Die Matrix erweitert sich folglich um $\lvert M_1 \rvert$ Zeilen. Für alle Randbedingungen der Form $x_j \gtrless 0$ definiere zwei Variablen:
\begin{align*}
 x_j^{+} &\colon= \begin{cases} %
x_j & \text{für } x_j > 0 \\
0 &\text{sonst}
\end{cases} \\
 x_j^{-} &\colon= \begin{cases} %
-x_j & \text{für } x_j < 0 \\
0 &\text{sonst}
\end{cases}
\end{align*}
Folglich gilt $x_j = x_j^{+} - x_j^{-}$. Die Bedingung $x_j \gtrless 0$ ist folglich äquivalent zu $x_j^{+} \geq 0 \land x_j^{-} \geq 0$. Die Matrix erweiter sich also nochmals um $\lvert N_1 \rvert$ Spalten (da wir eine Variable mehr haben) und der Vektor $x$ um ebensoviele Komponenten. Das entstandene Problem ist in kanonischer Form.
\item{\textit{allgemeine in Standardform}} \newline
siehe Übung
\end{enumerate}
\end{proof}

\section{Standard Form}
Wir betrachten zunächst ein Problem in Standardform unter folgenden Vorraussetzungen:
\begin{enumerate}[(a)]
	\item $rang(A) = m$\label{vorr:a}
	\item die Menge der zulässigen Punkte $F = \{x| Ax=b, x\geq0\}$ ist nicht leer\label{vorr:b}
	\item die Menge der Werte der Zielfunktion $\{c^T|x \in F\}$ ist nach unten beschränkt\label{vorr:c}
\end{enumerate}
\begin{definition}
Eine \Begriff{Basis} $B$ von $A$ ist eine Auswahl von linear unabhängigen Spaltenvektoren aus $A$. Alternativ entspricht $B$ einer regulären $m\times n$-Matrix.
Die \Begriff{Basislösung} zu $B$ ist ein Vektor $x\in \mathbb R^n$ mit 
\begin{align*}
x_{j_k} &= \begin{cases}
t_k &\text{wenn }A_{j_k} \in B \\
0 &\text{sonst}
\end{cases}\\
t &= B^{-1}b
\end{align*}
\end{definition}

Eine Basislösung $x$ zu einer Matrix $A$ können wir wie folgt berechnen:
\begin{enumerate}
\item Wähle eine Basis $B$ von $A$
\item Setze alle Komponenten von $x$, die nicht zu den gewählten Spalten aus $B$ gehören auf $0$.
\item Löse das resultierende Gleichungssystem, um die restlichen Komponenten von $x$ zu bestimmen.
\end{enumerate}

\begin{lemma}
Es sei $x = (x_1,\dotsc,x_n)$ eine Basislösung, $\alpha = \max\limits_{i\in \laue{m},j\in \laue{n}}\{\lvert a_{i,j} \rvert\}$ und $\beta = \max\limits_{j\in \laue{n}}\{\lvert b_j \rvert\}$. Dann gilt $\lvert x_j \rvert \leq m!\alpha^{m-1}\beta$ und $x_j \in \mathbb Q$.
\end{lemma}
\begin{proof}
Für eine Nicht-Basiskomponente gilt die Aussage, da diese per Definition $0$ sind. Die Basiskomponente $x_j$ ist die Summe von $m$ Produkten von Elementen von $B^{-1}$ und $b$. Nach Definition der Inversen gilt:
\begin{align*}
B^{-1} = \frac{1}{\det B}\adj(B) &= \frac{1}{\det B} \begin{pmatrix}
\mathcal B_{1,1} & \dots & \mathcal B_{1,m} \\
\vdots & \ & \vdots \\
\mathcal B_{m,1} & \dots & \mathcal B_{m,m}
\end{pmatrix}
\end{align*}
wobei $\mathcal B_{i,j}$ das Produkt von $(-1)^{i+j}$ und der Determinanten der Matrix ist, die durch Streichen der $i$-tem Zeile und $j$-ten Spalte aus $B$ entsteht.
Aufgrund des Entwicklungssatzes für Determinanten ist $\mathcal B_{i,j}$ die Summe von $(m-1)!$ Produkten von $m-1$ Elementen aus $A$. Somit gilt:
\begin{align*}
\lvert \mathcal B_{i,j} \rvert &\leq (m-1)!\alpha^{m-1} &&\text{da alle Elemente von $A$ $\leq \alpha$ sind}\\
\lvert x_j \rvert &= \left \lvert  \sum_{i \in \laue{m}} B^{-1}_{i,j} b_i \right \rvert &&\text{nach Definition}\\
&=\frac{1}{\det B}\left \lvert  \sum_{i \in \laue{m}} \mathcal B_{i,j} b_i \right \rvert \\
&\leq\frac{1}{\det B} \left \lvert  \sum_{i \in \laue{m}} (m-1)!\alpha^{m-1} b_i \right \rvert\\
&=\frac{1}{\det B}\left \lvert  m \cdot (m-1)!\alpha^{m-1} \cdot \beta \right \rvert \\
&=\frac{1}{\det B} m \cdot (m-1)!\alpha^{m-1} \cdot \beta &&\text{da alle Faktoren $\geq 0$ sind}
\end{align*}
Da $\det B$ ganzzahlig ist, folgt $\det B \geq 1$.
\end{proof}

\begin{theorem}
Unter der Vorraussetzungen \ref{vorr:a} und \ref{vorr:b} existert mind. eine Basislösung.
\end{theorem}
\begin{proof}
\textbf{Annahme:} Es existiert eine Lösung $x \in F$ mit $t > m$ Nicht-Null-Komponenten und es gibt keine Lösung $x' \in F$ mit weniger Nicht-Null-Komponenten.\newline\newline
Wir können O.B.d.A. annehmen, dass die ersten $t$ Komponenten von $x$ größer als $0$ sind und die restlichen Komponenten gleich $0$ sind (wenn nicht können wir dies durch Vertauschung erreichen).
Es gilt also $x_1,\dotsc,x_t > 0$ und $x_{t+1}, \dotsc,x_n=0$.
Hierraus folgt:
\begin{align}
b &= A_1x_1 + \dotsc + A_nx_n = A_1x_1 + \dotsc + A_tx_t \label{eq:x_n_t}
\end{align}
Es sei nun $r$ der Rang der Matrix $\lbrack A1, \dotsc, A_t\rbrack$. Wenn $r = 0$ wäre, so wäre $\vec 0$ eine zulässige Basislösung mit weniger Nicht-Null-Komponenten als $x$. Damit die Annahme stimmen kann muss also $0 < r \leq m < t$ gelten.
Mit dem Gauß-Jordan-Verfahren lassen sich nun die ersten $r$ Zeilen und Spalten in eine reguläre Form überführen. Das Problem sieht nun wie folgt aus:
\begin{align*}
\begin{pmatrix}
1 & & 0 & -\overline{a}_{1,r+1} &\hdots & -\overline{a}_{1,t} \\
 & \ddots & & \vdots & \ddots & \vdots \\
0 & & 1 & -\overline{a}_{r,r+1} &\hdots & -\overline{a}_{r,t} \\
? &&& \hdots && ?\\
\vdots &&&&&\\
?&&&&&\\
\end{pmatrix}\begin{pmatrix}
x_1\\
\vdots\\
x_r\\
\vdots
\end{pmatrix}&=\begin{pmatrix}
\overline{b}_1\\
\vdots\\
\overline{b}_r\\
\vdots
\end{pmatrix}
\end{align*}

Es lässt sich nun \eqref{eq:x_n_t} schreiben als:
\begin{align}
x_j &= \overline{b_j} + \sum_{i=r+1}^t \overline{a_{j,i}} x_i  &\text{für } j &={1,\dotsc,r}
\end{align}
Setze $\Theta = \min( x_{r+1}, \Theta_1)$ mit $\Theta_1 = \min( \frac{x_j}{\overline{a}_{j,r+1}}, j=1,\dotsc,r, \overline{a}_{j,r+1}>0 )$.
Konstruiere eine neue Lösung $\hat{x}$:
\begin{align}
\hat{x}_j &= \begin{cases}
\overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x_i} & \text{für } j < r+1 \\
x_j - \Theta & \text{für }  j=r+1\\
x_j & \text{für }  j>r+1
\end{cases}
\end{align}
Diese erfüllt das Gleichungssystem.
Dann gilt für $j<r$:
\begin{align}
\hat{x}_j &= \overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x}_i && j < r+1 \\
&= \overline{b}_j + \hat{a}_{j,r+1}(x_{r+1} - \Theta ) + \sum_{i=r+1}^t \overline{a}_{j,i} \hat{x}_i && j < r+1 \\
&= \underbrace{ \overline{b}_j + \sum_{i=r+1}^t \overline{a}_{j,i}\hat{x}_i }_{x_j} - \Theta\overline{a}_{j,r+1}
\end{align}
Ist $\Theta = x_{r+1}$, so ist $\hat{x}_{r+1} = 0$. Ist $\Theta = \Theta_1 = \frac{x_k}{\overline{a}_{k,r+1}}$. mit $k \leq r$, so ist %
 $\hat{x}_k = x_k - \Theta \overline{a}_{k,r+1} = 0$.
Für die Zulässigkeit ist noch zu zeigen: $\hat{x}_j \geq 0 \forall j \leq r + 1$.
Sehen wir uns $\hat{x}_{r+1}$ an:
Nach Definition: $\hat{x}_{r+1} = x_{r+1} - \Theta \geq \Theta - \Theta = 0$
\begin{align}
\hat{x}_j &= x_j - \Theta\overline{a}_{j,r+1} \\
&\geq \begin{cases}
 x_j + \Theta \lvert \overline{a}_{j,r+1} \rvert > 0 & \overline{a}_{j,r+1} \leq 0 \\
 x_j - \frac{x_j}{\overline{a}_{j,r+1}} \overline{a}_{j,r+1} = 0 & \overline{a}_{j,r+1} > 0
\end{cases}
\end{align}
Insgesamt ist $\hat{x}$ eine zulässige Lösung mit einer Nicht-Nullkomponente weniger als $x$, was ein Widerspruch zur Annahme ist.
\end{proof}
Es gibt also eine Lösung $x \in F$ mit $t\leq m$ Nicht-Nullkomponentent. O.B.d.A sind die zugehörigen Spalten linear unabhängig ( ansonsten Arg. oben mit Elimination von Variablen bzw. Gleich 0 setzen wiederholen ).
Falls $t < m$ ist, erweitere die Spalten zu einer Basis für $x$. ( Austauschsatz von Steinitz )

\begin{theorem}
Unter der Vorraussetzungen \ref{vorr:a},\ref{vorr:b} und \ref{vorr:c} ist das Lineare Programm $\min(c^Tx), Ax=b, x \geq 0$ äquivalent zu $\min(c^Tx), Ax=b, x \geq 0, x\leq M$ wobei %
$M = (m+1)!\alpha^m\beta$ mit $\alpha = max(\lvert a_{i,j}\rvert,\lvert c_{j}\rvert), \beta = max(\lvert b_i\rvert, \lvert z\rvert)$ und $z$ die Größte untere Schranke von $\{c^Tx | Ax=b, x \geq 0\}$ ist. 
\end{theorem}

\begin{quote}
In diesem Fall ist äquivalent zu verstehen als Gleichheit der Lösungsmengen. Es ist also jede Lösung des ersten Problems auch eine Lösung des zweiten Problems und umgekehrt.
\end{quote}

\section{Geometrie von Linearen Programmen}

Ein \Begriff{linearer Teilraum} S des $\mathbb R^d$ ist eine Teilmenge von $\mathbb R^b$, die
bezüglich der Vektoraddition und Skalarmultiplikation abgeschlossen ist.
Ein \Begriff{affiner Teilraum} A des $\mathbb R^d$ ist ein linearer Teilraum $S$,
verschoben um einen Vektor $u \in \mathbb R^d$: $A=\{u+x|x \in S\}$.

Die Dimension eines linearen Teilraumes S ist gleich der maximale Zahl von linear unabhängigen
Vektoren in S. Ebenso bei affinen Teilräumen. Die Dimension irgendeiner Menge X ist die kleinste
Dimension eines affinen Teilraums, der X enthält. Äquivalent können wir einen affinen bzw. linearen
Teilraum des $\mathbb R^d$ wie folgt darstellen:
$A = \{ x \in \mathbb R^d | a_i^Tx = b_i , 1 \leq i \leq m \}$
bzw. $S = \{ x \in \mathbb R^d | a_i^Tx = 0 , 1 \leq i \leq m \}$.

Zum Beispiel hat eine Kante die Dimension 1 und ein Menge von k Punkten höchstens die Dimension k-1.
Ein affiner Teilraum des $\mathbb R^d$ der Dimension $d-1$ wird als \Begriff{Hyperebene} bezeichnet.
Alternativ ist dies eine Menge von Punkten $H=\{x \in \mathbb R^d |a^Tx=b\}$ mit $a \neq 0$.
Eine Hyperebene definiert zwei \Begriff{abgeschlossene Halbräume}
$H^{+} = \{x \in \mathbb R^d |a^Tx \geq b\}$ und $H^{-} = \{x \in \mathbb R^d |a^Tx \leq b\}$.
Der Durchschnitt von endl. vielen Halbräumen wird bezeichnet als ein \Begriff{Polyeder}.
Ein Polyeder heißt \Begriff{Polytop}, wenn es beschränkt ist.

Es sei $P$ ein Polytop der Dimension $d$ im $\mathbb R^d$ und es sein $H \in \mathbb R^d$ eine Hyperebene. So ist $F = P \cap H$ eine Seitenfläche von $P$, wenn $H$ mindestens einen Punkt gemeinsam mit $P$ hat und wenn $P$ in höchstens einem der beiden Halbräumen $H^{+}$ und $H^{-}$ liegt. Es gibt drei wichtige Fälle:
\begin{enumerate}
\item eine \Begriff{Facette}, d.h. eine Seitenfläche der Dimension $d-1$.
\item eine \Begriff{Ecke}, d.h. eine Seitenfläche der Dimension $0$.
\item eine \Begriff{Kante}, d.h. eine Seitenfläche der Dimension $1$.
\end{enumerate}
Beispiel: ein Würfel im $\mathbb R^3$ ist gegeben durch $P = \{(x_1,x_2,x_3) | 0 \leq x_i \leq 1, i=1,2,3\}$. Dieser Würfel hat 6 Facetten, 8 Ecken und 12 Kanten.
\begin{align*}
F_1 &= P \cap \{(x_1,x_2,x_3) | x_3 = 1 \} && \text{Facette} \\
F_2 &= P \cap \{(x_1,x_2,x_3) | x_1 - x_3 = 1 \} && \text{Kante} \\
F_3 &= P \cap \{(x_1,x_2,x_3) | x_1 + x_2 + x_3 = 1 \} && \text{Ecke}
\end{align*}
\subsection{Umwandlung}
\begin{theorem}
\label{theorem:polytop_huelle}
\begin{enumerate}
\item Jedes Polytop ist die konvexe Hülle seiner Ecken.
\item Ist $V$ eine endliche Menge von Punkten/Vektoren, so ist die konvexe Hülle von $V$ ein Polytop.
\end{enumerate}
\end{theorem}
\begin{definition}
\Begriff{konvexe Hülle}
\begin{align*}
conv(V) &= \Big \{ \sum_{i=1}^k \lambda_i v_i \Big | v_i \in V, \lambda_i \geq 0, \sum_{i=1}^k \lambda_i = 1 \Big \}
\end{align*}
\end{definition}
Wir sind hauptsächlich interessiert an Polytopne, die im positiven Orthanten liegen.
D.h. die ersten d-Halbräume sind definiert durch $x_j \geq 0 ( j = 1,\dotsc,d)$.
Wegen \ref{theorem:polytop_huelle} kann ein Polytop $P$ auf verschiedene Weisen dargestellt werden:
\begin{enumerate}[a)]
\item als konvexe Hülle einer endlichen Punktemenge
\item als Durchschnitt von endlich vielen Halbräumen (der zusätzlich beschränkt ist)
\item als zulässiger Bereich eines LP-Problems (algebraische Darstellung)
\end{enumerate}
\paragraph*{Umwandlung LP-Problem in Durchschnitt von Halbräumen}
Es sei $F = \{ x | Ax = b , x \geq 0 \}$ der zulässige Bereich eines LP-Problems. Wir setzen vorraus, dass die Bedingungen \ref{vorr:a},\ref{vorr:b} und \ref{vorr:c} erfüllt sind. Da $rang(A) = m$ gilt, können wir O.B.d.A. annehmen, dass die Gleichungen $Ax=b$ in der folgenden Form vorliegen:
\begin{align}
x_{i+n-m} &= b_i - \sum_{j=1}^{n-m} a_{i,j}x_j & i = 1,\dotsc,m
\end{align}
Die Umformung erfolgt über Gauß-Jordan. Die Matrix enthält also einen Einheitsmatrix-Bereich im rechts-oberen Teil. Wegen der Vorraussetzung, dass $x_{i+n-m} \geq 0$ ist $Ax=b, x \geq 0$ äquivalent zu:
\begin{align}
 b_i - \sum_{j=1}^{n-m} a_{i,j}x_j &\geq 0 & i = 1,\dotsc,m\\
 x_j &\geq 0 & i = 1,\dotsc,n-m
\end{align}
Das ist der Durchschnitt von $n$ Halbräumen und ein Polytop im $\mathbb R^{n-m}$.
\paragraph*{Umwandlung  Durchschnitt von Halbräumen in LP-Problem}
Die $n$ Halbräume, die $P$ bestimmen seinen
\begin{align*}
h_{i,1} x_1 + \dotsc + h_{i,n-m} x_{n-m} &\leq g_i & i = 1,\dotsc,n
\end{align*}
Nach unserer Vorraussetzung sind die ersten $n-m$ Ungleichungen von der Form $x_i \geq 0, i = 1,\dotsc,n-m$.
Verwende positive Schlupfvariablen $x_{n-m+1}, \dotsc , x_n$ und erhalte:
\begin{align*}
h_{i,1} x_1 + \dotsc + h_{i,n-m} x_{n-m} + x_i &= g_i & i = n-m+i,\dotsc,n
\end{align*} 
Nun kann man dieses Problem als LP-Problem schreiben. Bilde dazu $A = ( H, I )$ (ein $m\times n$-Matrix) und erhalte das folgende System:
\begin{align*}
A &= \begin{pmatrix}
h_{1,1} & \hdots & h_{1,n-m} & 1 && 0\\
\vdots & \ddots & \vdots && \ddots & \\
h_{i,1} & \hdots & h_{i,n-m} & 0 && 1
\end{pmatrix}\\
Ax &= b\\
x &\geq 0\\
b &= (g_{n-m+1},\dotsc,g_n)^T
\end{align*}
\begin{definition}\label{def:umwandlung_punkt_vektor}
Der $x$-Vektor in den Halbraumdurchschnitten hatte nur die Dimension $n-m$, der $x$-Vektor des LP-Problems hatte die Dimension $n$. 
Jeder Punkt $\hat{x} \in (x_1,...,x_{n-m})^T \in P$ kann transformiert werden zu einem Punkt aus $F$ mit $n$ Komponenten. Dazu muss man lediglich die Schlupfvariablen ausrechnen:
\begin{align*}
x_i &= g_i - \sum_{j=1}^{n-m} h_{i,j}x_j & \text{für $i=n-m+1,\dotsc,n$}
\end{align*}
Ungekehrt wird ein $x\in F$ durch weglassen der letzten $m$ Komponenten zu einem Punkt $\hat{x} \in P$.
\end{definition}
\begin{theorem}
Es sei $P$ ein nicht-leeres Polytop, $F=\{x | Ax = b, x\geq 0\}$ der zugehörige zulässige Bereich eines LP und $\hat{x} = (x_1,\dotsc,x_{n-m})^T \in P$. Dann sind äquivalent:
\begin{enumerate}[a)]
\item Der Punkt $\hat{x}$ ist eine Ecke von $P$.
\item Wenn $\hat{x} = \lambda \hat{x}^1 + (1-\lambda)\hat{x}^2$ mit $\hat{x}^1,\hat{x}^2 \in P$ und $0 < \lambda < 1$, dann gillt $\hat{x} = \hat{x}^1 = \hat{x}^2$.
\item Der zugeordnete Vektor $x \in \mathbb R^n$, definiert durch \ref{def:umwandlung_punkt_vektor} ist eine zulässige Basislösung von $F$.
\end{enumerate}
\end{theorem}
\begin{proof}

\textbf{a $\Rightarrow$ b:} siehe Übung

\textbf{b $\Rightarrow$ c:}
$\hat{x}$ habe die Eigenschaft $b$.
Betrachte den zugeordneten Punkt $x \in F$ und die Menge $B= \{A_j | x_j > 0; j=1,\dotsc,n \}$.
Wir wollen zeigen, dass $B$ eine linear unabhängige Menge von Spalten ist. \newline
\textit{Annahme:} $B$ ist eine linear abhängige Menge von Spaltenvektoren.\newline
Dann gibt es Zahlen $\lambda_j$ ( nicht alle 0 ) mit $\sum_{A_j \in B} \lambda_j A_j = 0$. Da $x \in F$ gilt, gilt  $\sum_{A_j \in B} x_j A_j = b$ und $x_j \geq 0$. Dann gilt für jedes $\theta \in \mathbb R$:
\begin{align*}
\sum_{A_j \in B} (x_j \pm \theta\lambda_j) A_j &= b
\end{align*}
Da $x_j > 0$ für alle $A_j \in B$, gibt es ein $\theta > 0$ mit $x_j \pm \theta \lambda_j \geq 0 \forall A_j \in B$. Dann gibt es aber zwei Punkte $x^1, x^2 \in F$, definiert durch:
\begin{align*}
x_j^i &= \begin{cases}
x_j + (-1)^i \theta \lambda & A_j \in B \\
0 &\text{sonst}
\end{cases}
& i=1,2
\end{align*}
Da gilt:
\begin{align*}
x &= \frac{1}{2} x^1 + \frac{1}{2} x^2 
\end{align*}
Für die zugeordneten Punkte $\hat{x}^1, \hat{x}^2 \in P$ gilt $
\hat{x} = \frac{1}{2} \hat{x}^1 + \frac{1}{2} \hat{x}^2$, da diese durch Weglassung von Komponenten entstehen. Jedoch gilt $\hat{x}^1 \neq \hat{x}^2$. Also ist $B$ eine Menge von linear unabhängigen Spaltenvektoren mit $\lvert B \rvert \leq m$ und kann zu einer Basis erweitert werden.

\textbf{c $\Rightarrow$ a:}
Wenn $y = (y_1, \dotsc, y_n)^T$ eine zulässige Basislösung von $Ax = b, x  \geq 0$ ist, so gibt es einen Kostenvektor $c$, so dass $y$ der eindeutige Lösungsvektor des Systems $c^Tx \leq c^Ty, Ax=b, x\geq 0$ ist. Wir benutzen \ref{def:umwandlung_punkt_vektor} um hierraus Bedingungen für Punkte aus dem Polytop $P$ zu gewinnen:
\begin{align*}
c^Tx &= \sum_{i=1}^n c_ix_i \\
&=\sum_{i=1}^{n-m} c_ix_i + \sum_{i=n-m+1}^n c_ix_i \\
&=\sum_{i=1}^{n-m} c_ix_i + \sum_{i=n-m+1}^n c_i \left ( g_i - \sum_{j=1}^{n-m} h_{i,j}x_j \right )
\end{align*}
Fassen wir die Koeffizienten von $x_j$ jeweils zusammen, so bekommen wir:
\begin{align*}
d_j &= c_j - \sum_{i=n-m+1}^n c_i h_{i,j}
\end{align*}
Es folgt dann, dass $\hat{y} = (y_1,\dotsc,y_{n-m})$ der eindeutige Punkt im $\mathbb R^{n-m}$ ist mit
\begin{align*}
d^T\hat{x} &\leq d^T\hat{y} \forall \hat{x} \in P
\end{align*}
Also ist $\hat{x}$ eine Ecke von $P$.
\begin{quotation}
Die Menge der $\hat{x}$, für die $d^T\hat{x} = d^T\hat{y}$ gilt, bilden eine Hyperebene. Da der Schnittpunkt dieser Hypereben mit $P$ exakt einen Punkt hat ( nämlich $\hat{y}$ ) und $P$ komplett auf einer Seite dieser Hyperebene liegt ( nämlich $d^T\hat{x} \geq d^T\hat{y} \forall \hat{x} \in P$ ), ist $\hat{y}$ eine Ecke vom $P$. 
\end{quotation}
\end{proof}

\section{Transformation von einer zulässigen Basislösung zu einer anderen}
Es sei $x^0$ eine zulässige Basislösung eines LP-Problems mit gegebener $m\times n$-Matrix $A$ und zugeordneter Basis $B$ mit
\begin{align*}
B&=\{A_{B(i)} | i=1,\dotsc,m\}\\
B(i) &\in \{1,\dotsc,n\} \text{geordnet}
\end{align*}
Die Basiskomponenten von $x^0$ seien mit $x_i^0$ bezeichnet. Dann folgt:
\begin{align*}
\sum_{i=1}^m x_i^0 A_{B(i)} = b , x_i^0 \geq 0
\end{align*}
Da die Spalten der Basis linear unabhängig sind, kann jede andere Spalte $A_j$, die nicht in der Basis ist, als nicht-triviale Kombination der Basisspalten dargestellt werden. Es gilt also:
\begin{align*}
\sum_{i=1}^m x_{i,j} A_{B(i)} &= A_j
\end{align*}
Beide Gleichungen ergeben mit $\theta > 0$:
\begin{align*}
\sum_{i=1}^m ( x_i^0 - \theta x_ij ) A_{B(i)} + \theta A_j &= b
\end{align*}
\begin{definition}
Eine Basislösung heißt \Begriff{entartet}, wenn sie eine Nullkomponente enthält.
\end{definition}
Wenn $x^0$ nicht entartet ist, gilt $x_i^0 > 0$ für $i=1,\dotsc,m$. Mit 
\begin{align}
\theta_0 &= \min\limits_{i \text{mit} x_{i,j} > 0} \frac{x_i^0}{x_{i,j}} \\
&= \frac{x_l^0}{x_{l,j}} \label{eq:min_austausch}
\end{align}
wird ( falls mindestens ein $x_{i,j} > 0$ ist ) mindestens eine Komponente $0$ gesetzt und die Zulässigkeit bleibt erhalten.

\paragraph*{Spezialfall 1:} $x^0$ ist entartet und das zugehörige $x_{i,j} > 0$, dann folgt: $\theta_0 = 0$. In diesem Fall ändert sich die Lösung nicht, obwohl man die Spalte $B(i)$ durch die Spalte $A_j$ ersetzen kann.

\paragraph*{Spezialfall 2:} Alle $x_{i,j} \leq 0$. In diesem Fall kann $\theta$ ohne den Verlust der Zulässigkeit beliebig groß gesetzt werden. D.h. $F$ ist unbeschränkt.

\paragraph*{Spezialfall 3:} Wenn das Minimum in \eqref{eq:min_austausch} für mehrere Indices angenommen wird, so ist die neue Basislösung entartet.

\begin{theorem}
Gegeben sei eine zulässige Basislösung $x^0$ mit Basiskomponenten $x_i^0$, ($i=1,\dotsc,m$) und Basis $B=\{A_{B(i)} | i=1,\dotsc,m\}$ und ein Index $j$ sodass $A_j \notin B$. Dann ist die neue zulässige Lösung, die durch \eqref{eq:min_austausch} und durch
\begin{align}
x_i^1 &= \begin{cases}
x_i^0 - \theta_0 x_{i,j} & i \neq l\\
\theta_0 & i = l\end{cases}
\end{align}
bestimmt ist, eine zulässige Basislösung mit
\begin{align*}
B' &= (B \setminus \{A_{B(l)}\} ) \cup \{A_j\}
\end{align*} vorrausgesetzt mindestens ein $x_{i,j} > 0$ existiert.
\end{theorem}
\begin{example}
\begin{align*}
3x_1 + 2x_2 + x_3 &= 1\\
5x_1 + x_2 + x_3 + x_4 &= 3\\
2x_1 + 5x_2 + x_3 + x_5 &= 4
\end{align*}
Dargestellt als Tableau:
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
1 & 3 & 2 & 1 & 0 & 0\\
3 & 5 & 1 & 1 & 1 & 0\\
4 & 2 & 5 & 1 & 0 & 1
\end{matrix}
\end{align*}
Durch elementare Zeilenumformungen erhalten wir eine Einheitsmatrix und damit eine zulässige Basislösung.
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
1 & 3 & 2 & 1 & 0 & 0\\
2 & 2 & -1 & 0 & 1 & 0\\
3 & -1 & 3 & 0 & 0 & 1
\end{matrix}
\end{align*}
\begin{quotation}
Die Werte der nullten Spalte wird werden mit $x_{j,0}$ bezeichnet.
\end{quotation}
Nun hat man als Basis $B = \{A_3, A_4, A_5\}$ und die $0$-te Spalte gibt die Werte der Basisvariablen an. Die Nicht-Basisspalten in dem Tableau enthalten genau die Zahlen $x_{i,j}$, so zu Beispiel
\begin{align*}
A_1 &= 3A_3 + 2 A_4 - 1 A_5\\
 &=\sum x_{i,j} A_{B(i)}
\end{align*}
Die notwendigen Berechnungen für den Basiswechsel können direkt in dem Tableau ausgeführt werden. Wenn wir zum Beispiel die Spalte $A_1$ in die Basis aufnehmen wollen, so bestimmen wir das $\theta_0$ mit \eqref{eq:min_austausch}:
\begin{align*}
\theta_0 &= \min \left ( \frac{1}{3}, \frac{2}{2} \right ) = \frac{1}{3} \\
l&= 1
\end{align*}
Wir ersetzen die Spalte $B(l) = B(1) = 3$ mit Spalte $j=1$, indem wir in Spate $1$ einen Einheitsvektor $e_1$ erzeugen. Das neue Tableau sieht wie folgt aus:
\begin{align*}
\begin{matrix}
 & x_1 & x_2 & x_3 & x_4 & x_5 \\
\frac{1}{3} & 1 & \frac{2}{3} & \frac{1}{3} & 0 & 0\\
\frac{4}{3} & 0 & -\frac{7}{3} & -\frac{2}{3} & 1 & 0\\
\frac{4}{3} & 0 & \frac{11}{3} & \frac{1}{3} & 0 & 1
\end{matrix}
\end{align*}
Die neue Basis ist nun $B' = \{ A_1, A_4, A_5 \}$. Wenn im allgemeinen $x_{i,j}$ und $x'_{i,j}$ die alten und neuen Tableauwerte sind, $B$ und $B'$ die alte und neue Basis und $x_{l,j}$ das Pivotelement ist, so sehen Updateformeln wie folgt aus:
\begin{align*}
x_{l,q}' &= \frac{x_{l,q}}{x_{l,j}} &\text{für } q=0,\dotsc,n \\
x_{i,q}' &= x_{i,q} - x'_{l,q} x_{i,j} &\text{für } i=1,\dotsc,m; i \neq l ; q = 0,\dotsc,n \\
B'(i) &= \begin{cases}
B(i) & \text{für } i \neq l \\
j & \text{für } i = l
\end{cases}
\end{align*}
\begin{quotation}
\textbf{Bem:} $B(i)$ besagt, wo der Vektor $e_i$ im Tableau steht.
\end{quotation}
\end{example}

\subsection{Effekt der Kostenänderung}
Es sein $x^0$ ein zulässige Basislösungund Kosten $z^0 = \sum_{i=1}^m x_i^0 c_{B(i)}$. Die Kostenänderung, für den Fall, dass die Spalte $A_j \notin B$ in die Basis gebracht wird, sieht wie folgt aus:
\begin{align*}
\sum_{i=1}^m (\underbrace{ x_i^0 - \theta x_ij }_{x_i^1}) A_{B(i)} + \underbrace{\theta}_{x_j^1} A_j &= b
\end{align*}
Für jede Einheit, die $x_j$ zusätzlich bekommt, muss ein Betrag von $x_{i,j}$ der Variablen $x_{B(i)}$ weggelassen für $i=1,\dotsc,m$. Die Kostenänderung,
\begin{align}
\overline{c_j} &= c_j - \sum_{i=1}^m x_{i,j} c_{B(i)} = c_j - z_j \label{eq:rel_kosten}
\end{align}
wird \Begriff{relative Kosten} der Spalte $j$ genannt. Dabei ist es günstig eine Spalte $j$ mit $\overline{c_j} < 0$ in die Basis aufzunehmen. Wenn alle $\overline{c} \geq 0$ sind, haben wir ein lokales Optimum gefunden. Dieses lokale Optimum ist sogar ein globales (wird später bewiesen).
Für irgendein Tableau $X$ sei $B$ die $m \times m$-Matrix, die die Spalten aus $A$ für die Basis in enthalten und es sei $c_B \in \mathbb Z^m$ die Kosten dieser Basisvariablen. Da wir $X$ durch diagonalisierung der Basisspalten von $A$ bekommen, gilt, dass $X=B^{-1}A$ und für den Vektor $z=(z_1,\dotsc,z_n)$ gilt nach \eqref{eq:rel_kosten}:
$z^T = c_B^TX = c_B^TB^{-1}A$.
\begin{theorem}
Bei gegebener zulässiger Basislösung $x^0$ ändern sich die Kosten, wenn die Variable $x_j$ durch einen Pivotschritt in die Basis aufgenommen wird, um den Betrag
\begin{align*}
\theta_0 \overline{c_j} = \theta_0 (c_j - z_j)
\end{align*}
\end{theorem}
Wenn $\overline{c} = c - z \geq 0$, so ist $x^0$ optimal.
